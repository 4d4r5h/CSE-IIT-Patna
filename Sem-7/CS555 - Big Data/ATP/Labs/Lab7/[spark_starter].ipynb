{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54d2f07a-b590-429c-8a82-2b05aa41405c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Working with Apache Spark using pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9ab886-b6fe-4bd8-9fdc-3c46791882c8",
   "metadata": {},
   "source": [
    "# ~ Install spark and pyspark\n",
    "\n",
    "Install spark on your server\n",
    "\n",
    "<code> sudo apt install spark </code>\n",
    "\n",
    "*spark can be accessed from command line using the spark-shell command (here the shell acts as a driver program)*\n",
    "\n",
    "<code> spark-shell -c spark.driver.bindAddress=127.0.0.1 </code>\n",
    "\n",
    "\n",
    "Install the pyspark package using pip\n",
    "\n",
    "<code> pip install pyspark </code>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a241d170-8637-4fcd-a0ee-7548cbf74f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyspark\n",
      "Version: 3.1.2\n",
      "Summary: Apache Spark Python API\n",
      "Home-page: https://github.com/apache/spark/tree/master/python\n",
      "Author: Spark Developers\n",
      "Author-email: dev@spark.apache.org\n",
      "License: http://www.apache.org/licenses/LICENSE-2.0\n",
      "Location: /home/iitp/anaconda3/lib/python3.8/site-packages\n",
      "Requires: py4j\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "107329b9-65e8-4d41-8865-559b430ad1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, os, shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655ea60c-565b-4f26-a92f-b92849e53f35",
   "metadata": {},
   "source": [
    "# Create spark session\n",
    "\n",
    "a spark session must be created before working with spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc683806-8dc6-4527-9a49-c215d140f384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://vmu1800:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Lab6</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcedbaceaf0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to start a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark = SparkSession.builder.appName('Lab6').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4f29e4-67d7-4a1f-8a03-c57891eef9fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# The *context* object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55bf700e-036a-4010-8763-36f23e3ae9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparkContext: <SparkContext master=local[*] appName=Lab6>\n"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "print('sparkContext:',sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b63a5a2-758c-432d-a081-6b6067c1f9c1",
   "metadata": {},
   "source": [
    "# RDD - Resilient Distributed Datasets\n",
    " RDDs are **immutable** collection of datasets that work in parallel<br>\n",
    "  read more@ https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds\n",
    " \n",
    "* **Transformations** are lazy operations on RDDs - stores actions (in a DAG) rather than actual transformation\n",
    "    * read more@ https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations\n",
    "* **Actions** on RDDs produce results (using the RDD DAG)\n",
    "    * read more@ https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b6c4bd8-f727-4760-8c97-c3779261c8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RDD in module pyspark.rdd:\n",
      "\n",
      "class RDD(builtins.object)\n",
      " |  RDD(jrdd, ctx, jrdd_deserializer=AutoBatchedSerializer(PickleSerializer()))\n",
      " |  \n",
      " |  A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.\n",
      " |  Represents an immutable, partitioned collection of elements that can be\n",
      " |  operated on in parallel.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |      Return the union of this RDD and another one.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n",
      " |      >>> (rdd + rdd).collect()\n",
      " |      [1, 1, 2, 3, 1, 1, 2, 3]\n",
      " |  \n",
      " |  __getnewargs__(self)\n",
      " |  \n",
      " |  __init__(self, jrdd, ctx, jrdd_deserializer=AutoBatchedSerializer(PickleSerializer()))\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  aggregate(self, zeroValue, seqOp, combOp)\n",
      " |      Aggregate the elements of each partition, and then the results for all\n",
      " |      the partitions, using a given combine functions and a neutral \"zero\n",
      " |      value.\"\n",
      " |      \n",
      " |      The functions ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n",
      " |      as its result value to avoid object allocation; however, it should not\n",
      " |      modify ``t2``.\n",
      " |      \n",
      " |      The first function (seqOp) can return a different result type, U, than\n",
      " |      the type of this RDD. Thus, we need one operation for merging a T into\n",
      " |      an U and one operation for merging two U\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
      " |      >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
      " |      >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n",
      " |      (10, 4)\n",
      " |      >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)\n",
      " |      (0, 0)\n",
      " |  \n",
      " |  aggregateByKey(self, zeroValue, seqFunc, combFunc, numPartitions=None, partitionFunc=<function portable_hash at 0x7fcedbed8670>)\n",
      " |      Aggregate the values of each key, using given combine functions and a neutral\n",
      " |      \"zero value\". This function can return a different result type, U, than the type\n",
      " |      of the values in this RDD, V. Thus, we need one operation for merging a V into\n",
      " |      a U and one operation for merging two U's, The former operation is used for merging\n",
      " |      values within a partition, and the latter is used for merging values between\n",
      " |      partitions. To avoid memory allocation, both of these functions are\n",
      " |      allowed to modify and return their first argument instead of creating a new U.\n",
      " |  \n",
      " |  barrier(self)\n",
      " |      Marks the current stage as a barrier stage, where Spark must launch all tasks together.\n",
      " |      In case of a task failure, instead of only restarting the failed task, Spark will abort the\n",
      " |      entire stage and relaunch all tasks for this stage.\n",
      " |      The barrier execution mode feature is experimental and it only handles limited scenarios.\n",
      " |      Please read the linked SPIP and design docs to understand the limitations and future plans.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDDBarrier`\n",
      " |          instance that provides actions within a barrier stage.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pyspark.BarrierTaskContext\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For additional information see\n",
      " |      \n",
      " |      - `SPIP: Barrier Execution Mode <http://jira.apache.org/jira/browse/SPARK-24374>`_\n",
      " |      - `Design Doc <https://jira.apache.org/jira/browse/SPARK-24582>`_\n",
      " |      \n",
      " |      This API is experimental\n",
      " |  \n",
      " |  cache(self)\n",
      " |      Persist this RDD with the default storage level (`MEMORY_ONLY`).\n",
      " |  \n",
      " |  cartesian(self, other)\n",
      " |      Return the Cartesian product of this RDD and another one, that is, the\n",
      " |      RDD of all pairs of elements ``(a, b)`` where ``a`` is in `self` and\n",
      " |      ``b`` is in `other`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1, 2])\n",
      " |      >>> sorted(rdd.cartesian(rdd).collect())\n",
      " |      [(1, 1), (1, 2), (2, 1), (2, 2)]\n",
      " |  \n",
      " |  checkpoint(self)\n",
      " |      Mark this RDD for checkpointing. It will be saved to a file inside the\n",
      " |      checkpoint directory set with :meth:`SparkContext.setCheckpointDir` and\n",
      " |      all references to its parent RDDs will be removed. This function must\n",
      " |      be called before any job has been executed on this RDD. It is strongly\n",
      " |      recommended that this RDD is persisted in memory, otherwise saving it\n",
      " |      on a file will require recomputation.\n",
      " |  \n",
      " |  coalesce(self, numPartitions, shuffle=False)\n",
      " |      Return a new RDD that is reduced into `numPartitions` partitions.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()\n",
      " |      [[1], [2, 3], [4, 5]]\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()\n",
      " |      [[1, 2, 3, 4, 5]]\n",
      " |  \n",
      " |  cogroup(self, other, numPartitions=None)\n",
      " |      For each key k in `self` or `other`, return a resulting RDD that\n",
      " |      contains a tuple with the list of values for that key in `self` as\n",
      " |      well as `other`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> [(x, tuple(map(list, y))) for x, y in sorted(list(x.cogroup(y).collect()))]\n",
      " |      [('a', ([1], [2])), ('b', ([4], []))]\n",
      " |  \n",
      " |  collect(self)\n",
      " |      Return a list that contains all of the elements in this RDD.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method should only be used if the resulting array is expected\n",
      " |      to be small, as all the data is loaded into the driver's memory.\n",
      " |  \n",
      " |  collectAsMap(self)\n",
      " |      Return the key-value pairs in this RDD to the master as a dictionary.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method should only be used if the resulting data is expected\n",
      " |      to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\n",
      " |      >>> m[1]\n",
      " |      2\n",
      " |      >>> m[3]\n",
      " |      4\n",
      " |  \n",
      " |  collectWithJobGroup(self, groupId, description, interruptOnCancel=False)\n",
      " |      When collect rdd, use this method to specify job group.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |      .. deprecated:: 3.1.0\n",
      " |          Use :class:`pyspark.InheritableThread` with the pinned thread mode enabled.\n",
      " |  \n",
      " |  combineByKey(self, createCombiner, mergeValue, mergeCombiners, numPartitions=None, partitionFunc=<function portable_hash at 0x7fcedbed8670>)\n",
      " |      Generic function to combine the elements for each key using a custom\n",
      " |      set of aggregation functions.\n",
      " |      \n",
      " |      Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a \"combined\n",
      " |      type\" C.\n",
      " |      \n",
      " |      Users provide three functions:\n",
      " |      \n",
      " |          - `createCombiner`, which turns a V into a C (e.g., creates\n",
      " |            a one-element list)\n",
      " |          - `mergeValue`, to merge a V into a C (e.g., adds it to the end of\n",
      " |            a list)\n",
      " |          - `mergeCombiners`, to combine two C's into a single one (e.g., merges\n",
      " |            the lists)\n",
      " |      \n",
      " |      To avoid memory allocation, both mergeValue and mergeCombiners are allowed to\n",
      " |      modify and return their first argument instead of creating a new C.\n",
      " |      \n",
      " |      In addition, users can control the partitioning of the output RDD.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      V and C can be different -- for example, one might group an RDD of type\n",
      " |          (Int, Int) into an RDD of type (Int, List[Int]).\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
      " |      >>> def to_list(a):\n",
      " |      ...     return [a]\n",
      " |      ...\n",
      " |      >>> def append(a, b):\n",
      " |      ...     a.append(b)\n",
      " |      ...     return a\n",
      " |      ...\n",
      " |      >>> def extend(a, b):\n",
      " |      ...     a.extend(b)\n",
      " |      ...     return a\n",
      " |      ...\n",
      " |      >>> sorted(x.combineByKey(to_list, append, extend).collect())\n",
      " |      [('a', [1, 2]), ('b', [1])]\n",
      " |  \n",
      " |  count(self)\n",
      " |      Return the number of elements in this RDD.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([2, 3, 4]).count()\n",
      " |      3\n",
      " |  \n",
      " |  countApprox(self, timeout, confidence=0.95)\n",
      " |      Approximate version of count() that returns a potentially incomplete\n",
      " |      result within a timeout, even if not all tasks have finished.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
      " |      >>> rdd.countApprox(1000, 1.0)\n",
      " |      1000\n",
      " |  \n",
      " |  countApproxDistinct(self, relativeSD=0.05)\n",
      " |      Return approximate number of distinct elements in the RDD.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      relativeSD : float, optional\n",
      " |          Relative accuracy. Smaller values create\n",
      " |          counters that require more space.\n",
      " |          It must be greater than 0.000017.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The algorithm used is based on streamlib's implementation of\n",
      " |      `\"HyperLogLog in Practice: Algorithmic Engineering of a State\n",
      " |      of The Art Cardinality Estimation Algorithm\", available here\n",
      " |      <https://doi.org/10.1145/2452376.2452456>`_.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> n = sc.parallelize(range(1000)).map(str).countApproxDistinct()\n",
      " |      >>> 900 < n < 1100\n",
      " |      True\n",
      " |      >>> n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()\n",
      " |      >>> 16 < n < 24\n",
      " |      True\n",
      " |  \n",
      " |  countByKey(self)\n",
      " |      Count the number of elements for each key, and return the result to the\n",
      " |      master as a dictionary.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.countByKey().items())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |  \n",
      " |  countByValue(self)\n",
      " |      Return the count of each unique value in this RDD as a dictionary of\n",
      " |      (value, count) pairs.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())\n",
      " |      [(1, 2), (2, 3)]\n",
      " |  \n",
      " |  distinct(self, numPartitions=None)\n",
      " |      Return a new RDD containing the distinct elements in this RDD.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n",
      " |      [1, 2, 3]\n",
      " |  \n",
      " |  filter(self, f)\n",
      " |      Return a new RDD containing only the elements that satisfy a predicate.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
      " |      >>> rdd.filter(lambda x: x % 2 == 0).collect()\n",
      " |      [2, 4]\n",
      " |  \n",
      " |  first(self)\n",
      " |      Return the first element in this RDD.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([2, 3, 4]).first()\n",
      " |      2\n",
      " |      >>> sc.parallelize([]).first()\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      ValueError: RDD is empty\n",
      " |  \n",
      " |  flatMap(self, f, preservesPartitioning=False)\n",
      " |      Return a new RDD by first applying a function to all elements of this\n",
      " |      RDD, and then flattening the results.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([2, 3, 4])\n",
      " |      >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n",
      " |      [1, 1, 1, 2, 2, 3]\n",
      " |      >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n",
      " |      [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n",
      " |  \n",
      " |  flatMapValues(self, f)\n",
      " |      Pass each value in the key-value pair RDD through a flatMap function\n",
      " |      without changing the keys; this also retains the original RDD's\n",
      " |      partitioning.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"r\"])])\n",
      " |      >>> def f(x): return x\n",
      " |      >>> x.flatMapValues(f).collect()\n",
      " |      [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n",
      " |  \n",
      " |  fold(self, zeroValue, op)\n",
      " |      Aggregate the elements of each partition, and then the results for all\n",
      " |      the partitions, using a given associative function and a neutral \"zero value.\"\n",
      " |      \n",
      " |      The function ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n",
      " |      as its result value to avoid object allocation; however, it should not\n",
      " |      modify ``t2``.\n",
      " |      \n",
      " |      This behaves somewhat differently from fold operations implemented\n",
      " |      for non-distributed collections in functional languages like Scala.\n",
      " |      This fold operation may be applied to partitions individually, and then\n",
      " |      fold those results into the final result, rather than apply the fold\n",
      " |      to each element sequentially in some defined ordering. For functions\n",
      " |      that are not commutative, the result may differ from that of a fold\n",
      " |      applied to a non-distributed collection.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from operator import add\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)\n",
      " |      15\n",
      " |  \n",
      " |  foldByKey(self, zeroValue, func, numPartitions=None, partitionFunc=<function portable_hash at 0x7fcedbed8670>)\n",
      " |      Merge the values for each key using an associative function \"func\"\n",
      " |      and a neutral \"zeroValue\" which may be added to the result an\n",
      " |      arbitrary number of times, and must not change the result\n",
      " |      (e.g., 0 for addition, or 1 for multiplication.).\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> from operator import add\n",
      " |      >>> sorted(rdd.foldByKey(0, add).collect())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |  \n",
      " |  foreach(self, f)\n",
      " |      Applies a function to all elements of this RDD.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> def f(x): print(x)\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)\n",
      " |  \n",
      " |  foreachPartition(self, f)\n",
      " |      Applies a function to each partition of this RDD.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> def f(iterator):\n",
      " |      ...     for x in iterator:\n",
      " |      ...          print(x)\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)\n",
      " |  \n",
      " |  fullOuterJoin(self, other, numPartitions=None)\n",
      " |      Perform a right outer join of `self` and `other`.\n",
      " |      \n",
      " |      For each element (k, v) in `self`, the resulting RDD will either\n",
      " |      contain all pairs (k, (v, w)) for w in `other`, or the pair\n",
      " |      (k, (v, None)) if no elements in `other` have key k.\n",
      " |      \n",
      " |      Similarly, for each element (k, w) in `other`, the resulting RDD will\n",
      " |      either contain all pairs (k, (v, w)) for v in `self`, or the pair\n",
      " |      (k, (None, w)) if no elements in `self` have key k.\n",
      " |      \n",
      " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2), (\"c\", 8)])\n",
      " |      >>> sorted(x.fullOuterJoin(y).collect())\n",
      " |      [('a', (1, 2)), ('b', (4, None)), ('c', (None, 8))]\n",
      " |  \n",
      " |  getCheckpointFile(self)\n",
      " |      Gets the name of the file to which this RDD was checkpointed\n",
      " |      \n",
      " |      Not defined if RDD is checkpointed locally.\n",
      " |  \n",
      " |  getNumPartitions(self)\n",
      " |      Returns the number of partitions in RDD\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      " |      >>> rdd.getNumPartitions()\n",
      " |      2\n",
      " |  \n",
      " |  getResourceProfile(self)\n",
      " |      Get the :class:`pyspark.resource.ResourceProfile` specified with this RDD or None\n",
      " |      if it wasn't specified.\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :py:class:`pyspark.resource.ResourceProfile`\n",
      " |          The the user specified profile or None if none were specified\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is experimental\n",
      " |  \n",
      " |  getStorageLevel(self)\n",
      " |      Get the RDD's current storage level.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd1 = sc.parallelize([1,2])\n",
      " |      >>> rdd1.getStorageLevel()\n",
      " |      StorageLevel(False, False, False, False, 1)\n",
      " |      >>> print(rdd1.getStorageLevel())\n",
      " |      Serialized 1x Replicated\n",
      " |  \n",
      " |  glom(self)\n",
      " |      Return an RDD created by coalescing all elements within each partition\n",
      " |      into a list.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      " |      >>> sorted(rdd.glom().collect())\n",
      " |      [[1, 2], [3, 4]]\n",
      " |  \n",
      " |  groupBy(self, f, numPartitions=None, partitionFunc=<function portable_hash at 0x7fcedbed8670>)\n",
      " |      Return an RDD of grouped items.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
      " |      >>> result = rdd.groupBy(lambda x: x % 2).collect()\n",
      " |      >>> sorted([(x, sorted(y)) for (x, y) in result])\n",
      " |      [(0, [2, 8]), (1, [1, 1, 3, 5])]\n",
      " |  \n",
      " |  groupByKey(self, numPartitions=None, partitionFunc=<function portable_hash at 0x7fcedbed8670>)\n",
      " |      Group the values for each key in the RDD into a single sequence.\n",
      " |      Hash-partitions the resulting RDD with numPartitions partitions.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If you are grouping in order to perform an aggregation (such as a\n",
      " |      sum or average) over each key, using reduceByKey or aggregateByKey will\n",
      " |      provide much better performance.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.groupByKey().mapValues(len).collect())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |      >>> sorted(rdd.groupByKey().mapValues(list).collect())\n",
      " |      [('a', [1, 1]), ('b', [1])]\n",
      " |  \n",
      " |  groupWith(self, other, *others)\n",
      " |      Alias for cogroup but with support for multiple RDDs.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> w = sc.parallelize([(\"a\", 5), (\"b\", 6)])\n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> z = sc.parallelize([(\"b\", 42)])\n",
      " |      >>> [(x, tuple(map(list, y))) for x, y in sorted(list(w.groupWith(x, y, z).collect()))]\n",
      " |      [('a', ([5], [1], [2], [])), ('b', ([6], [4], [], [42]))]\n",
      " |  \n",
      " |  histogram(self, buckets)\n",
      " |      Compute a histogram using the provided buckets. The buckets\n",
      " |      are all open to the right except for the last which is closed.\n",
      " |      e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],\n",
      " |      which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1\n",
      " |      and 50 we would have a histogram of 1,0,1.\n",
      " |      \n",
      " |      If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),\n",
      " |      this can be switched from an O(log n) insertion to O(1) per\n",
      " |      element (where n is the number of buckets).\n",
      " |      \n",
      " |      Buckets must be sorted, not contain any duplicates, and have\n",
      " |      at least two elements.\n",
      " |      \n",
      " |      If `buckets` is a number, it will generate buckets which are\n",
      " |      evenly spaced between the minimum and maximum of the RDD. For\n",
      " |      example, if the min value is 0 and the max is 100, given `buckets`\n",
      " |      as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must\n",
      " |      be at least 1. An exception is raised if the RDD contains infinity.\n",
      " |      If the elements in the RDD do not vary (max == min), a single bucket\n",
      " |      will be used.\n",
      " |      \n",
      " |      The return value is a tuple of buckets and histogram.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize(range(51))\n",
      " |      >>> rdd.histogram(2)\n",
      " |      ([0, 25, 50], [25, 26])\n",
      " |      >>> rdd.histogram([0, 5, 25, 50])\n",
      " |      ([0, 5, 25, 50], [5, 20, 26])\n",
      " |      >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets\n",
      " |      ([0, 15, 30, 45, 60], [15, 15, 15, 6])\n",
      " |      >>> rdd = sc.parallelize([\"ab\", \"ac\", \"b\", \"bd\", \"ef\"])\n",
      " |      >>> rdd.histogram((\"a\", \"b\", \"c\"))\n",
      " |      (('a', 'b', 'c'), [2, 2])\n",
      " |  \n",
      " |  id(self)\n",
      " |      A unique ID for this RDD (within its SparkContext).\n",
      " |  \n",
      " |  intersection(self, other)\n",
      " |      Return the intersection of this RDD and another one. The output will\n",
      " |      not contain any duplicate elements, even if the input RDDs did.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method performs a shuffle internally.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n",
      " |      >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n",
      " |      >>> rdd1.intersection(rdd2).collect()\n",
      " |      [1, 2, 3]\n",
      " |  \n",
      " |  isCheckpointed(self)\n",
      " |      Return whether this RDD is checkpointed and materialized, either reliably or locally.\n",
      " |  \n",
      " |  isEmpty(self)\n",
      " |      Returns true if and only if the RDD contains no elements at all.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      An RDD may be empty even when it has at least 1 partition.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([]).isEmpty()\n",
      " |      True\n",
      " |      >>> sc.parallelize([1]).isEmpty()\n",
      " |      False\n",
      " |  \n",
      " |  isLocallyCheckpointed(self)\n",
      " |      Return whether this RDD is marked for local checkpointing.\n",
      " |      \n",
      " |      Exposed for testing.\n",
      " |  \n",
      " |  join(self, other, numPartitions=None)\n",
      " |      Return an RDD containing all pairs of elements with matching keys in\n",
      " |      `self` and `other`.\n",
      " |      \n",
      " |      Each pair of elements will be returned as a (k, (v1, v2)) tuple, where\n",
      " |      (k, v1) is in `self` and (k, v2) is in `other`.\n",
      " |      \n",
      " |      Performs a hash join across the cluster.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
      " |      >>> sorted(x.join(y).collect())\n",
      " |      [('a', (1, 2)), ('a', (1, 3))]\n",
      " |  \n",
      " |  keyBy(self, f)\n",
      " |      Creates tuples of the elements in this RDD by applying `f`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)\n",
      " |      >>> y = sc.parallelize(zip(range(0,5), range(0,5)))\n",
      " |      >>> [(x, list(map(list, y))) for x, y in sorted(x.cogroup(y).collect())]\n",
      " |      [(0, [[0], [0]]), (1, [[1], [1]]), (2, [[], [2]]), (3, [[], [3]]), (4, [[2], [4]])]\n",
      " |  \n",
      " |  keys(self)\n",
      " |      Return an RDD with the keys of each tuple.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> m = sc.parallelize([(1, 2), (3, 4)]).keys()\n",
      " |      >>> m.collect()\n",
      " |      [1, 3]\n",
      " |  \n",
      " |  leftOuterJoin(self, other, numPartitions=None)\n",
      " |      Perform a left outer join of `self` and `other`.\n",
      " |      \n",
      " |      For each element (k, v) in `self`, the resulting RDD will either\n",
      " |      contain all pairs (k, (v, w)) for w in `other`, or the pair\n",
      " |      (k, (v, None)) if no elements in `other` have key k.\n",
      " |      \n",
      " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> sorted(x.leftOuterJoin(y).collect())\n",
      " |      [('a', (1, 2)), ('b', (4, None))]\n",
      " |  \n",
      " |  localCheckpoint(self)\n",
      " |      Mark this RDD for local checkpointing using Spark's existing caching layer.\n",
      " |      \n",
      " |      This method is for users who wish to truncate RDD lineages while skipping the expensive\n",
      " |      step of replicating the materialized data in a reliable distributed file system. This is\n",
      " |      useful for RDDs with long lineages that need to be truncated periodically (e.g. GraphX).\n",
      " |      \n",
      " |      Local checkpointing sacrifices fault-tolerance for performance. In particular, checkpointed\n",
      " |      data is written to ephemeral local storage in the executors instead of to a reliable,\n",
      " |      fault-tolerant storage. The effect is that if an executor fails during the computation,\n",
      " |      the checkpointed data may no longer be accessible, causing an irrecoverable job failure.\n",
      " |      \n",
      " |      This is NOT safe to use with dynamic allocation, which removes executors along\n",
      " |      with their cached blocks. If you must use both features, you are advised to set\n",
      " |      `spark.dynamicAllocation.cachedExecutorIdleTimeout` to a high value.\n",
      " |      \n",
      " |      The checkpoint directory set through :meth:`SparkContext.setCheckpointDir` is not used.\n",
      " |  \n",
      " |  lookup(self, key)\n",
      " |      Return the list of values in the RDD for key `key`. This operation\n",
      " |      is done efficiently if the RDD has a known partitioner by only\n",
      " |      searching the partition that the key maps to.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> l = range(1000)\n",
      " |      >>> rdd = sc.parallelize(zip(l, l), 10)\n",
      " |      >>> rdd.lookup(42)  # slow\n",
      " |      [42]\n",
      " |      >>> sorted = rdd.sortByKey()\n",
      " |      >>> sorted.lookup(42)  # fast\n",
      " |      [42]\n",
      " |      >>> sorted.lookup(1024)\n",
      " |      []\n",
      " |      >>> rdd2 = sc.parallelize([(('a', 'b'), 'c')]).groupByKey()\n",
      " |      >>> list(rdd2.lookup(('a', 'b'))[0])\n",
      " |      ['c']\n",
      " |  \n",
      " |  map(self, f, preservesPartitioning=False)\n",
      " |      Return a new RDD by applying a function to each element of this RDD.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
      " |      >>> sorted(rdd.map(lambda x: (x, 1)).collect())\n",
      " |      [('a', 1), ('b', 1), ('c', 1)]\n",
      " |  \n",
      " |  mapPartitions(self, f, preservesPartitioning=False)\n",
      " |      Return a new RDD by applying a function to each partition of this RDD.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      " |      >>> def f(iterator): yield sum(iterator)\n",
      " |      >>> rdd.mapPartitions(f).collect()\n",
      " |      [3, 7]\n",
      " |  \n",
      " |  mapPartitionsWithIndex(self, f, preservesPartitioning=False)\n",
      " |      Return a new RDD by applying a function to each partition of this RDD,\n",
      " |      while tracking the index of the original partition.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
      " |      >>> def f(splitIndex, iterator): yield splitIndex\n",
      " |      >>> rdd.mapPartitionsWithIndex(f).sum()\n",
      " |      6\n",
      " |  \n",
      " |  mapPartitionsWithSplit(self, f, preservesPartitioning=False)\n",
      " |      Return a new RDD by applying a function to each partition of this RDD,\n",
      " |      while tracking the index of the original partition.\n",
      " |      \n",
      " |      .. deprecated:: 0.9.0\n",
      " |          use :py:meth:`RDD.mapPartitionsWithIndex` instead.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
      " |      >>> def f(splitIndex, iterator): yield splitIndex\n",
      " |      >>> rdd.mapPartitionsWithSplit(f).sum()\n",
      " |      6\n",
      " |  \n",
      " |  mapValues(self, f)\n",
      " |      Pass each value in the key-value pair RDD through a map function\n",
      " |      without changing the keys; this also retains the original RDD's\n",
      " |      partitioning.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])\n",
      " |      >>> def f(x): return len(x)\n",
      " |      >>> x.mapValues(f).collect()\n",
      " |      [('a', 3), ('b', 1)]\n",
      " |  \n",
      " |  max(self, key=None)\n",
      " |      Find the maximum item in this RDD.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : function, optional\n",
      " |          A function used to generate key for comparing\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])\n",
      " |      >>> rdd.max()\n",
      " |      43.0\n",
      " |      >>> rdd.max(key=str)\n",
      " |      5.0\n",
      " |  \n",
      " |  mean(self)\n",
      " |      Compute the mean of this RDD's elements.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([1, 2, 3]).mean()\n",
      " |      2.0\n",
      " |  \n",
      " |  meanApprox(self, timeout, confidence=0.95)\n",
      " |      Approximate operation to return the mean within a timeout\n",
      " |      or meet the confidence.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
      " |      >>> r = sum(range(1000)) / 1000.0\n",
      " |      >>> abs(rdd.meanApprox(1000) - r) / r < 0.05\n",
      " |      True\n",
      " |  \n",
      " |  min(self, key=None)\n",
      " |      Find the minimum item in this RDD.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : function, optional\n",
      " |          A function used to generate key for comparing\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])\n",
      " |      >>> rdd.min()\n",
      " |      2.0\n",
      " |      >>> rdd.min(key=str)\n",
      " |      10.0\n",
      " |  \n",
      " |  name(self)\n",
      " |      Return the name of this RDD.\n",
      " |  \n",
      " |  partitionBy(self, numPartitions, partitionFunc=<function portable_hash at 0x7fcedbed8670>)\n",
      " |      Return a copy of the RDD partitioned using the specified partitioner.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))\n",
      " |      >>> sets = pairs.partitionBy(2).glom().collect()\n",
      " |      >>> len(set(sets[0]).intersection(set(sets[1])))\n",
      " |      0\n",
      " |  \n",
      " |  persist(self, storageLevel=StorageLevel(False, True, False, False, 1))\n",
      " |      Set this RDD's storage level to persist its values across operations\n",
      " |      after the first time it is computed. This can only be used to assign\n",
      " |      a new storage level if the RDD does not have a storage level set yet.\n",
      " |      If no storage level is specified defaults to (`MEMORY_ONLY`).\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
      " |      >>> rdd.persist().is_cached\n",
      " |      True\n",
      " |  \n",
      " |  pipe(self, command, env=None, checkCode=False)\n",
      " |      Return an RDD created by piping elements to a forked external process.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      command : str\n",
      " |          command to run.\n",
      " |      env : dict, optional\n",
      " |          environment variables to set.\n",
      " |      checkCode : bool, optional\n",
      " |          whether or not to check the return value of the shell command.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()\n",
      " |      ['1', '2', '', '3']\n",
      " |  \n",
      " |  randomSplit(self, weights, seed=None)\n",
      " |      Randomly splits this RDD with the provided weights.\n",
      " |      \n",
      " |      weights : list\n",
      " |          weights for splits, will be normalized if they don't sum to 1\n",
      " |      seed : int, optional\n",
      " |          random seed\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          split RDDs in a list\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize(range(500), 1)\n",
      " |      >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)\n",
      " |      >>> len(rdd1.collect() + rdd2.collect())\n",
      " |      500\n",
      " |      >>> 150 < rdd1.count() < 250\n",
      " |      True\n",
      " |      >>> 250 < rdd2.count() < 350\n",
      " |      True\n",
      " |  \n",
      " |  reduce(self, f)\n",
      " |      Reduces the elements of this RDD using the specified commutative and\n",
      " |      associative binary operator. Currently reduces partitions locally.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from operator import add\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)\n",
      " |      15\n",
      " |      >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)\n",
      " |      10\n",
      " |      >>> sc.parallelize([]).reduce(add)\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      ValueError: Can not reduce() empty RDD\n",
      " |  \n",
      " |  reduceByKey(self, func, numPartitions=None, partitionFunc=<function portable_hash at 0x7fcedbed8670>)\n",
      " |      Merge the values for each key using an associative and commutative reduce function.\n",
      " |      \n",
      " |      This will also perform the merging locally on each mapper before\n",
      " |      sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n",
      " |      \n",
      " |      Output will be partitioned with `numPartitions` partitions, or\n",
      " |      the default parallelism level if `numPartitions` is not specified.\n",
      " |      Default partitioner is hash-partition.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from operator import add\n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.reduceByKey(add).collect())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |  \n",
      " |  reduceByKeyLocally(self, func)\n",
      " |      Merge the values for each key using an associative and commutative reduce function, but\n",
      " |      return the results immediately to the master as a dictionary.\n",
      " |      \n",
      " |      This will also perform the merging locally on each mapper before\n",
      " |      sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from operator import add\n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.reduceByKeyLocally(add).items())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |  \n",
      " |  repartition(self, numPartitions)\n",
      " |       Return a new RDD that has exactly numPartitions partitions.\n",
      " |      \n",
      " |       Can increase or decrease the level of parallelism in this RDD.\n",
      " |       Internally, this uses a shuffle to redistribute data.\n",
      " |       If you are decreasing the number of partitions in this RDD, consider\n",
      " |       using `coalesce`, which can avoid performing a shuffle.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |       >>> rdd = sc.parallelize([1,2,3,4,5,6,7], 4)\n",
      " |       >>> sorted(rdd.glom().collect())\n",
      " |       [[1], [2, 3], [4, 5], [6, 7]]\n",
      " |       >>> len(rdd.repartition(2).glom().collect())\n",
      " |       2\n",
      " |       >>> len(rdd.repartition(10).glom().collect())\n",
      " |       10\n",
      " |  \n",
      " |  repartitionAndSortWithinPartitions(self, numPartitions=None, partitionFunc=<function portable_hash at 0x7fcedbed8670>, ascending=True, keyfunc=<function RDD.<lambda> at 0x7fcedbcfc160>)\n",
      " |      Repartition the RDD according to the given partitioner and, within each resulting partition,\n",
      " |      sort records by their keys.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\n",
      " |      >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)\n",
      " |      >>> rdd2.glom().collect()\n",
      " |      [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]\n",
      " |  \n",
      " |  rightOuterJoin(self, other, numPartitions=None)\n",
      " |      Perform a right outer join of `self` and `other`.\n",
      " |      \n",
      " |      For each element (k, w) in `other`, the resulting RDD will either\n",
      " |      contain all pairs (k, (v, w)) for v in this, or the pair (k, (None, w))\n",
      " |      if no elements in `self` have key k.\n",
      " |      \n",
      " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> sorted(y.rightOuterJoin(x).collect())\n",
      " |      [('a', (2, 1)), ('b', (None, 4))]\n",
      " |  \n",
      " |  sample(self, withReplacement, fraction, seed=None)\n",
      " |      Return a sampled subset of this RDD.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      withReplacement : bool\n",
      " |          can elements be sampled multiple times (replaced when sampled out)\n",
      " |      fraction : float\n",
      " |          expected size of the sample as a fraction of this RDD's size\n",
      " |          without replacement: probability that each element is chosen; fraction must be [0, 1]\n",
      " |          with replacement: expected number of times each element is chosen; fraction must be >= 0\n",
      " |      seed : int, optional\n",
      " |          seed for the random number generator\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This is not guaranteed to provide exactly the fraction specified of the total\n",
      " |      count of the given :class:`DataFrame`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize(range(100), 4)\n",
      " |      >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14\n",
      " |      True\n",
      " |  \n",
      " |  sampleByKey(self, withReplacement, fractions, seed=None)\n",
      " |      Return a subset of this RDD sampled by key (via stratified sampling).\n",
      " |      Create a sample of this RDD using variable sampling rates for\n",
      " |      different keys as specified by fractions, a key to sampling rate map.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> fractions = {\"a\": 0.2, \"b\": 0.1}\n",
      " |      >>> rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))\n",
      " |      >>> sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())\n",
      " |      >>> 100 < len(sample[\"a\"]) < 300 and 50 < len(sample[\"b\"]) < 150\n",
      " |      True\n",
      " |      >>> max(sample[\"a\"]) <= 999 and min(sample[\"a\"]) >= 0\n",
      " |      True\n",
      " |      >>> max(sample[\"b\"]) <= 999 and min(sample[\"b\"]) >= 0\n",
      " |      True\n",
      " |  \n",
      " |  sampleStdev(self)\n",
      " |      Compute the sample standard deviation of this RDD's elements (which\n",
      " |      corrects for bias in estimating the standard deviation by dividing by\n",
      " |      N-1 instead of N).\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([1, 2, 3]).sampleStdev()\n",
      " |      1.0\n",
      " |  \n",
      " |  sampleVariance(self)\n",
      " |      Compute the sample variance of this RDD's elements (which corrects\n",
      " |      for bias in estimating the variance by dividing by N-1 instead of N).\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([1, 2, 3]).sampleVariance()\n",
      " |      1.0\n",
      " |  \n",
      " |  saveAsHadoopDataset(self, conf, keyConverter=None, valueConverter=None)\n",
      " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
      " |      system, using the old Hadoop OutputFormat API (mapred package). Keys/values are\n",
      " |      converted for output using either user specified converters or, by default,\n",
      " |      \"org.apache.spark.api.python.JavaToWritableConverter\".\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      conf : dict\n",
      " |          Hadoop job configuration\n",
      " |      keyConverter : str, optional\n",
      " |          fully qualified classname of key converter (None by default)\n",
      " |      valueConverter : str, optional\n",
      " |          fully qualified classname of value converter (None by default)\n",
      " |  \n",
      " |  saveAsHadoopFile(self, path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None, compressionCodecClass=None)\n",
      " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
      " |      system, using the old Hadoop OutputFormat API (mapred package). Key and value types\n",
      " |      will be inferred if not specified. Keys and values are converted for output using either\n",
      " |      user specified converters or \"org.apache.spark.api.python.JavaToWritableConverter\". The\n",
      " |      `conf` is applied on top of the base Hadoop conf associated with the SparkContext\n",
      " |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          path to Hadoop file\n",
      " |      outputFormatClass : str\n",
      " |          fully qualified classname of Hadoop OutputFormat\n",
      " |          (e.g. \"org.apache.hadoop.mapred.SequenceFileOutputFormat\")\n",
      " |      keyClass : str, optional\n",
      " |          fully qualified classname of key Writable class\n",
      " |          (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n",
      " |      valueClass : str, optional\n",
      " |          fully qualified classname of value Writable class\n",
      " |          (e.g. \"org.apache.hadoop.io.Text\", None by default)\n",
      " |      keyConverter : str, optional\n",
      " |          fully qualified classname of key converter (None by default)\n",
      " |      valueConverter : str, optional\n",
      " |          fully qualified classname of value converter (None by default)\n",
      " |      conf : dict, optional\n",
      " |          (None by default)\n",
      " |      compressionCodecClass : str\n",
      " |          fully qualified classname of the compression codec class\n",
      " |          i.e. \"org.apache.hadoop.io.compress.GzipCodec\" (None by default)\n",
      " |  \n",
      " |  saveAsNewAPIHadoopDataset(self, conf, keyConverter=None, valueConverter=None)\n",
      " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
      " |      system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are\n",
      " |      converted for output using either user specified converters or, by default,\n",
      " |      \"org.apache.spark.api.python.JavaToWritableConverter\".\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      conf : dict\n",
      " |          Hadoop job configuration\n",
      " |      keyConverter : str, optional\n",
      " |          fully qualified classname of key converter (None by default)\n",
      " |      valueConverter : str, optional\n",
      " |          fully qualified classname of value converter (None by default)\n",
      " |  \n",
      " |  saveAsNewAPIHadoopFile(self, path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None)\n",
      " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
      " |      system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types\n",
      " |      will be inferred if not specified. Keys and values are converted for output using either\n",
      " |      user specified converters or \"org.apache.spark.api.python.JavaToWritableConverter\". The\n",
      " |      `conf` is applied on top of the base Hadoop conf associated with the SparkContext\n",
      " |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n",
      " |      \n",
      " |      path : str\n",
      " |          path to Hadoop file\n",
      " |      outputFormatClass : str\n",
      " |          fully qualified classname of Hadoop OutputFormat\n",
      " |          (e.g. \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\")\n",
      " |      keyClass : str, optional\n",
      " |          fully qualified classname of key Writable class\n",
      " |           (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n",
      " |      valueClass : str, optional\n",
      " |          fully qualified classname of value Writable class\n",
      " |          (e.g. \"org.apache.hadoop.io.Text\", None by default)\n",
      " |      keyConverter : str, optional\n",
      " |          fully qualified classname of key converter (None by default)\n",
      " |      valueConverter : str, optional\n",
      " |          fully qualified classname of value converter (None by default)\n",
      " |      conf : dict, optional\n",
      " |          Hadoop job configuration (None by default)\n",
      " |  \n",
      " |  saveAsPickleFile(self, path, batchSize=10)\n",
      " |      Save this RDD as a SequenceFile of serialized objects. The serializer\n",
      " |      used is :class:`pyspark.serializers.PickleSerializer`, default batch size\n",
      " |      is 10.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from tempfile import NamedTemporaryFile\n",
      " |      >>> tmpFile = NamedTemporaryFile(delete=True)\n",
      " |      >>> tmpFile.close()\n",
      " |      >>> sc.parallelize([1, 2, 'spark', 'rdd']).saveAsPickleFile(tmpFile.name, 3)\n",
      " |      >>> sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())\n",
      " |      ['1', '2', 'rdd', 'spark']\n",
      " |  \n",
      " |  saveAsSequenceFile(self, path, compressionCodecClass=None)\n",
      " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
      " |      system, using the \"org.apache.hadoop.io.Writable\" types that we convert from the\n",
      " |      RDD's key and value types. The mechanism is as follows:\n",
      " |      \n",
      " |          1. Pyrolite is used to convert pickled Python RDD into RDD of Java objects.\n",
      " |          2. Keys and values of this Java RDD are converted to Writables and written out.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          path to sequence file\n",
      " |      compressionCodecClass : str, optional\n",
      " |          fully qualified classname of the compression codec class\n",
      " |          i.e. \"org.apache.hadoop.io.compress.GzipCodec\" (None by default)\n",
      " |  \n",
      " |  saveAsTextFile(self, path, compressionCodecClass=None)\n",
      " |      Save this RDD as a text file, using string representations of elements.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          path to text file\n",
      " |      compressionCodecClass : str, optional\n",
      " |          fully qualified classname of the compression codec class\n",
      " |          i.e. \"org.apache.hadoop.io.compress.GzipCodec\" (None by default)\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from tempfile import NamedTemporaryFile\n",
      " |      >>> tempFile = NamedTemporaryFile(delete=True)\n",
      " |      >>> tempFile.close()\n",
      " |      >>> sc.parallelize(range(10)).saveAsTextFile(tempFile.name)\n",
      " |      >>> from fileinput import input\n",
      " |      >>> from glob import glob\n",
      " |      >>> ''.join(sorted(input(glob(tempFile.name + \"/part-0000*\"))))\n",
      " |      '0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n'\n",
      " |      \n",
      " |      Empty lines are tolerated when saving to text files.\n",
      " |      \n",
      " |      >>> from tempfile import NamedTemporaryFile\n",
      " |      >>> tempFile2 = NamedTemporaryFile(delete=True)\n",
      " |      >>> tempFile2.close()\n",
      " |      >>> sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(tempFile2.name)\n",
      " |      >>> ''.join(sorted(input(glob(tempFile2.name + \"/part-0000*\"))))\n",
      " |      '\\n\\n\\nbar\\nfoo\\n'\n",
      " |      \n",
      " |      Using compressionCodecClass\n",
      " |      \n",
      " |      >>> from tempfile import NamedTemporaryFile\n",
      " |      >>> tempFile3 = NamedTemporaryFile(delete=True)\n",
      " |      >>> tempFile3.close()\n",
      " |      >>> codec = \"org.apache.hadoop.io.compress.GzipCodec\"\n",
      " |      >>> sc.parallelize(['foo', 'bar']).saveAsTextFile(tempFile3.name, codec)\n",
      " |      >>> from fileinput import input, hook_compressed\n",
      " |      >>> result = sorted(input(glob(tempFile3.name + \"/part*.gz\"), openhook=hook_compressed))\n",
      " |      >>> b''.join(result).decode('utf-8')\n",
      " |      'bar\\nfoo\\n'\n",
      " |  \n",
      " |  setName(self, name)\n",
      " |      Assign a name to this RDD.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd1 = sc.parallelize([1, 2])\n",
      " |      >>> rdd1.setName('RDD1').name()\n",
      " |      'RDD1'\n",
      " |  \n",
      " |  sortBy(self, keyfunc, ascending=True, numPartitions=None)\n",
      " |      Sorts this RDD by the given keyfunc\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
      " |      >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()\n",
      " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      " |      >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()\n",
      " |      [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
      " |  \n",
      " |  sortByKey(self, ascending=True, numPartitions=None, keyfunc=<function RDD.<lambda> at 0x7fcedbcfc280>)\n",
      " |      Sorts this RDD, which is assumed to consist of (key, value) pairs.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
      " |      >>> sc.parallelize(tmp).sortByKey().first()\n",
      " |      ('1', 3)\n",
      " |      >>> sc.parallelize(tmp).sortByKey(True, 1).collect()\n",
      " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      " |      >>> sc.parallelize(tmp).sortByKey(True, 2).collect()\n",
      " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      " |      >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
      " |      >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
      " |      >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n",
      " |      [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]\n",
      " |  \n",
      " |  stats(self)\n",
      " |      Return a :class:`StatCounter` object that captures the mean, variance\n",
      " |      and count of the RDD's elements in one operation.\n",
      " |  \n",
      " |  stdev(self)\n",
      " |      Compute the standard deviation of this RDD's elements.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([1, 2, 3]).stdev()\n",
      " |      0.816...\n",
      " |  \n",
      " |  subtract(self, other, numPartitions=None)\n",
      " |      Return each value in `self` that is not contained in `other`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
      " |      >>> sorted(x.subtract(y).collect())\n",
      " |      [('a', 1), ('b', 4), ('b', 5)]\n",
      " |  \n",
      " |  subtractByKey(self, other, numPartitions=None)\n",
      " |      Return each (key, value) pair in `self` that has no pair with matching\n",
      " |      key in `other`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 2)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
      " |      >>> sorted(x.subtractByKey(y).collect())\n",
      " |      [('b', 4), ('b', 5)]\n",
      " |  \n",
      " |  sum(self)\n",
      " |      Add up the elements in this RDD.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([1.0, 2.0, 3.0]).sum()\n",
      " |      6.0\n",
      " |  \n",
      " |  sumApprox(self, timeout, confidence=0.95)\n",
      " |      Approximate operation to return the sum within a timeout\n",
      " |      or meet the confidence.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
      " |      >>> r = sum(range(1000))\n",
      " |      >>> abs(rdd.sumApprox(1000) - r) / r < 0.05\n",
      " |      True\n",
      " |  \n",
      " |  take(self, num)\n",
      " |      Take the first num elements of the RDD.\n",
      " |      \n",
      " |      It works by first scanning one partition, and use the results from\n",
      " |      that partition to estimate the number of additional partitions needed\n",
      " |      to satisfy the limit.\n",
      " |      \n",
      " |      Translated from the Scala implementation in RDD#take().\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method should only be used if the resulting array is expected\n",
      " |      to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\n",
      " |      [2, 3]\n",
      " |      >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)\n",
      " |      [2, 3, 4, 5, 6]\n",
      " |      >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)\n",
      " |      [91, 92, 93]\n",
      " |  \n",
      " |  takeOrdered(self, num, key=None)\n",
      " |      Get the N elements from an RDD ordered in ascending order or as\n",
      " |      specified by the optional key function.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method should only be used if the resulting array is expected\n",
      " |      to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n",
      " |      [1, 2, 3, 4, 5, 6]\n",
      " |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n",
      " |      [10, 9, 7, 6, 5, 4]\n",
      " |  \n",
      " |  takeSample(self, withReplacement, num, seed=None)\n",
      " |      Return a fixed-size sampled subset of this RDD.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method should only be used if the resulting array is expected\n",
      " |      to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize(range(0, 10))\n",
      " |      >>> len(rdd.takeSample(True, 20, 1))\n",
      " |      20\n",
      " |      >>> len(rdd.takeSample(False, 5, 2))\n",
      " |      5\n",
      " |      >>> len(rdd.takeSample(False, 15, 3))\n",
      " |      10\n",
      " |  \n",
      " |  toDF(self, schema=None, sampleRatio=None)\n",
      " |      Converts current :class:`RDD` into a :class:`DataFrame`\n",
      " |      \n",
      " |      This is a shorthand for ``spark.createDataFrame(rdd, schema, sampleRatio)``\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
      " |          a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      " |          column names, default is None.  The data type string format equals to\n",
      " |          :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      " |          omit the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use\n",
      " |          ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`.\n",
      " |          We can also use ``int`` as a short name for :class:`pyspark.sql.types.IntegerType`.\n",
      " |      sampleRatio : float, optional\n",
      " |          the sample ratio of rows used for inferring\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd.toDF().collect()\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |  \n",
      " |  toDebugString(self)\n",
      " |      A description of this RDD and its recursive dependencies for debugging.\n",
      " |  \n",
      " |  toLocalIterator(self, prefetchPartitions=False)\n",
      " |      Return an iterator that contains all of the elements in this RDD.\n",
      " |      The iterator will consume as much memory as the largest partition in this RDD.\n",
      " |      With prefetch it may consume up to the memory of the 2 largest partitions.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      prefetchPartitions : bool, optional\n",
      " |          If Spark should pre-fetch the next partition\n",
      " |          before it is needed.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize(range(10))\n",
      " |      >>> [x for x in rdd.toLocalIterator()]\n",
      " |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      " |  \n",
      " |  top(self, num, key=None)\n",
      " |      Get the top N elements from an RDD.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method should only be used if the resulting array is expected\n",
      " |      to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      It returns the list sorted in descending order.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)\n",
      " |      [12]\n",
      " |      >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)\n",
      " |      [6, 5]\n",
      " |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)\n",
      " |      [4, 3, 2]\n",
      " |  \n",
      " |  treeAggregate(self, zeroValue, seqOp, combOp, depth=2)\n",
      " |      Aggregates the elements of this RDD in a multi-level tree\n",
      " |      pattern.\n",
      " |      \n",
      " |      depth : int, optional\n",
      " |          suggested depth of the tree (default: 2)\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> add = lambda x, y: x + y\n",
      " |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
      " |      >>> rdd.treeAggregate(0, add, add)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 1)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 2)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 5)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 10)\n",
      " |      -5\n",
      " |  \n",
      " |  treeReduce(self, f, depth=2)\n",
      " |      Reduces the elements of this RDD in a multi-level tree pattern.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |      depth : int, optional\n",
      " |          suggested depth of the tree (default: 2)\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> add = lambda x, y: x + y\n",
      " |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
      " |      >>> rdd.treeReduce(add)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 1)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 2)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 5)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 10)\n",
      " |      -5\n",
      " |  \n",
      " |  union(self, other)\n",
      " |      Return the union of this RDD and another one.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n",
      " |      >>> rdd.union(rdd).collect()\n",
      " |      [1, 1, 2, 3, 1, 1, 2, 3]\n",
      " |  \n",
      " |  unpersist(self, blocking=False)\n",
      " |      Mark the RDD as non-persistent, and remove all blocks for it from\n",
      " |      memory and disk.\n",
      " |      \n",
      " |      .. versionchanged:: 3.0.0\n",
      " |         Added optional argument `blocking` to specify whether to block until all\n",
      " |         blocks are deleted.\n",
      " |  \n",
      " |  values(self)\n",
      " |      Return an RDD with the values of each tuple.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> m = sc.parallelize([(1, 2), (3, 4)]).values()\n",
      " |      >>> m.collect()\n",
      " |      [2, 4]\n",
      " |  \n",
      " |  variance(self)\n",
      " |      Compute the variance of this RDD's elements.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([1, 2, 3]).variance()\n",
      " |      0.666...\n",
      " |  \n",
      " |  withResources(self, profile)\n",
      " |      Specify a :class:`pyspark.resource.ResourceProfile` to use when calculating this RDD.\n",
      " |      This is only supported on certain cluster managers and currently requires dynamic\n",
      " |      allocation to be enabled. It will result in new executors with the resources specified\n",
      " |      being acquired to calculate the RDD.\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is experimental\n",
      " |  \n",
      " |  zip(self, other)\n",
      " |      Zips this RDD with another one, returning key-value pairs with the\n",
      " |      first element in each RDD second element in each RDD, etc. Assumes\n",
      " |      that the two RDDs have the same number of partitions and the same\n",
      " |      number of elements in each partition (e.g. one was made through\n",
      " |      a map on the other).\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = sc.parallelize(range(0,5))\n",
      " |      >>> y = sc.parallelize(range(1000, 1005))\n",
      " |      >>> x.zip(y).collect()\n",
      " |      [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]\n",
      " |  \n",
      " |  zipWithIndex(self)\n",
      " |      Zips this RDD with its element indices.\n",
      " |      \n",
      " |      The ordering is first based on the partition index and then the\n",
      " |      ordering of items within each partition. So the first item in\n",
      " |      the first partition gets index 0, and the last item in the last\n",
      " |      partition receives the largest index.\n",
      " |      \n",
      " |      This method needs to trigger a spark job when this RDD contains\n",
      " |      more than one partitions.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\"], 3).zipWithIndex().collect()\n",
      " |      [('a', 0), ('b', 1), ('c', 2), ('d', 3)]\n",
      " |  \n",
      " |  zipWithUniqueId(self)\n",
      " |      Zips this RDD with generated unique Long ids.\n",
      " |      \n",
      " |      Items in the kth partition will get ids k, n+k, 2*n+k, ..., where\n",
      " |      n is the number of partitions. So there may exist gaps, but this\n",
      " |      method won't trigger a spark job, which is different from\n",
      " |      :meth:`zipWithIndex`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"], 3).zipWithUniqueId().collect()\n",
      " |      [('a', 0), ('b', 1), ('c', 4), ('d', 2), ('e', 5)]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  context\n",
      " |      The :class:`SparkContext` that this RDD was created on.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pyspark.RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2c79b8-d26c-40b3-b536-19b17c7470ac",
   "metadata": {},
   "source": [
    "# Creating RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447bcdda-57da-45da-b767-570268a6ad72",
   "metadata": {},
   "source": [
    "## creating from sc.parallelize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37ab2ce1-e6a3-459e-9ba0-d290327ab9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD_array1 = sc.parallelize(np.random.randint(0,10,size=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828b0efc-12ef-4b8e-8188-b2061cbfbd22",
   "metadata": {},
   "source": [
    "## apply transformation and actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7216ecb-befb-4cdc-93de-2b61f2f8c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD_array2 = RDD_array1.map(lambda x: x*2) # 'map' is a transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e15015d7-4ded-474b-859b-a778a1be7a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 9, 2, 0, 4, 1, 2, 0, 5, 0] [14, 18, 4, 0, 8, 2, 4, 0, 10, 0]\n"
     ]
    }
   ],
   "source": [
    "a1 = RDD_array1.collect() # 'collect' is an action\n",
    "a2 = RDD_array2.collect()\n",
    "print( a1,  a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c68f54c2-8311-4602-93b0-57c8c3a94c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "a3 = RDD_array2.reduce(lambda x,y: x+y)  # 'reduce' is an action\n",
    "print(a3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc5b417-2df6-47ef-9fcc-c91297a410a4",
   "metadata": {},
   "source": [
    "## creating from files or external objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af86d3dd-827c-4408-8b00-90738e27fed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD_F = sc.textFile(\"Apple_stock.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96cdc620-9db6-427c-8e13-fb78a8a401d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 1597\n",
      "<class 'list'> 1597\n",
      "Samples\n",
      " <class 'list'> 3 ['2015-11-16,28.559999465942383,27.75,27.844999313354492,28.545000076293945,152426800.0,26.322452545166016', '2011-01-03,11.795000076293945,11.601428985595703,11.630000114440918,11.770357131958008,445138400.0,10.106220245361328', '2012-09-10,24.403213500976562,23.64642906188965,24.301786422729492,23.669286727905273,487998000.0,20.410099029541016']\n",
      "Header: Date,High,Low,Open,Close,Volume,Adj Close\n"
     ]
    }
   ],
   "source": [
    "cf = RDD_F.count() # 'count' is an action\n",
    "print('Count:',cf)\n",
    "\n",
    "data = RDD_F.collect() # 'collect' is an action\n",
    "print(type(data), len(data))\n",
    "\n",
    "samples = RDD_F.takeSample(True, 3)  # takeSample is an action\n",
    "print('Samples\\n', type(samples), len(samples), samples)\n",
    "\n",
    "first = RDD_F.first()\n",
    "print('Header:',first) #<---- shall be used for schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feadf886-60cc-4c13-96bf-15004cac4d9f",
   "metadata": {},
   "source": [
    "# mapper function used to map values to data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c22c318a-455a-461d-b97c-1e41ea6feb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(xs):\n",
    "    global first\n",
    "    if xs!=first:\n",
    "        x = xs.split(\",\")\n",
    "        return [str(x[0]), float(x[1]), float(x[2]), float(x[3]), float(x[4]), float(x[5]), float(x[6])]\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100b2373-fbc7-45a7-9716-cdb0cdfe18a4",
   "metadata": {},
   "source": [
    "## create data frame using mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17cda365-abd0-4d12-8279-6753171ab352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'> 1596\n",
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: double (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n",
      "+----------+-----------------+-----------------+-----------------+-----------------+----------+------------------+\n",
      "|      Date|             High|              Low|             Open|            Close|    Volume|         Adj Close|\n",
      "+----------+-----------------+-----------------+-----------------+-----------------+----------+------------------+\n",
      "|2010-08-02|9.378213882446289|9.272143363952637| 9.30142879486084|9.351785659790039|4.280556E8| 8.029596328735352|\n",
      "|2010-08-03|9.402142524719238|9.265000343322754|9.321785926818848|9.354642868041992|4.176536E8| 8.032052993774414|\n",
      "|2010-08-04|9.438570976257324|9.296786308288574|  9.3871431350708|9.392143249511719|4.203752E8| 8.064249038696289|\n",
      "|2010-08-05|9.399286270141602|9.305356979370117| 9.34749984741211|9.346428871154785|2.890972E8| 8.024996757507324|\n",
      "|2010-08-06|9.338929176330566|9.201070785522461|9.277856826782227|9.288928985595703|4.448976E8| 7.975627899169922|\n",
      "|2010-08-09|9.362500190734863|9.270357131958008|9.338570594787598|9.348214149475098| 3.03128E8| 8.026529312133789|\n",
      "|2010-08-10|9.301786422729492|9.198213577270508|9.280357360839844|9.264642715454102|  4.5192E8| 7.954774856567383|\n",
      "|2010-08-11|9.131786346435547|8.921786308288574|9.121429443359375|8.935357093811035|6.200544E8| 7.672046184539795|\n",
      "|2010-08-12|9.039285659790039|8.789999961853027|8.810357093811035|8.992500305175781|5.349204E8|7.7211103439331055|\n",
      "|2010-08-13| 8.99571418762207|8.896071434020996|8.987500190734863|8.896429061889648|3.548692E8| 7.638622283935547|\n",
      "|2010-08-16|8.928929328918457|8.807856559753418|8.842143058776855| 8.84428596496582|  3.1843E8|7.5938496589660645|\n",
      "|2010-08-17|9.093929290771484|8.899999618530273|8.931428909301758|8.998929023742676|4.226404E8| 7.726629257202148|\n",
      "|2010-08-18|9.095356941223145|8.984999656677246|9.012857437133789|9.038213729858398| 3.39696E8|7.7603607177734375|\n",
      "|2010-08-19|9.052857398986816|8.881428718566895|9.029999732971191|8.924285888671875| 4.26706E8| 7.662538051605225|\n",
      "|2010-08-20|9.068571090698242| 8.89285659790039| 8.90678596496582|8.915714263916016|  3.8423E8| 7.655179500579834|\n",
      "|2010-08-23|              9.0|8.758929252624512|8.992500305175781|8.778571128845215|4.140416E8|7.5374250411987305|\n",
      "|2010-08-24|8.678570747375488|8.523214340209961|8.666786193847656|8.568928718566895|6.025656E8| 7.357422828674316|\n",
      "|2010-08-25|8.713929176330566|8.471428871154785|8.501428604125977|8.674642562866211|5.968676E8| 7.448192596435547|\n",
      "|2010-08-26|8.776785850524902|8.581428527832031|8.766071319580078|8.581428527832031|4.665052E8| 7.368154048919678|\n",
      "|2010-08-27|8.664643287658691|8.412857055664062|8.633929252624512| 8.62928581237793|5.483912E8| 7.409246921539307|\n",
      "+----------+-----------------+-----------------+-----------------+-----------------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = RDD_F.map(mapper).filter(lambda x: x!=None).toDF(first.split(','))\n",
    "print(type(dataset), dataset.count())\n",
    "dataset.printSchema()\n",
    "dataset.show() # or use .describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdff1e4-e181-4a16-823d-4d6f3202216d",
   "metadata": {},
   "source": [
    "# Linear Regression Example\n",
    "\n",
    "*Not using spark.mllib*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd9c0f23-545c-4287-bf94-51cc3950d89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# a line is y = mx + c where m and c are the params\n",
    "ground_m, ground_c = random.randint(0,5), random.randint(0,5) # the ground truth\n",
    "\n",
    "def line(x):\n",
    "    global ground_m, ground_c\n",
    "    return ground_m*x + ground_c\n",
    "\n",
    "# generate at least 100 samples and save to a file \n",
    "rdd_x = sc.parallelize(np.random.randint(0,1000,size=100))\n",
    "rdd_y = rdd_x.map(line)\n",
    "\n",
    "# generate input data and save to file\n",
    "data_x, data_y = rdd_x.collect(), rdd_y.collect() #<-- 'collect' is an action\n",
    "plt.scatter(data_x, data_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d685e776-cce1-4546-8d59-d1170c1f74a7",
   "metadata": {},
   "source": [
    "## Estimate using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "648be528-09d2-468d-8177-ed7e3ff084bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Model  # always use this approach instead of Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "\n",
    "# create a single layer model\n",
    "input_layer = Input((1,))\n",
    "output_layer = Dense(1)(input_layer)\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss='mse', optimizer=Adam(learning_rate=0.1), metrics=[])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1619352e-f481-4bfa-a106-5029d873a24f",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a600097-25af-4cc9-a225-224c27887afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fce94424c40>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lossv=[]\n",
    "for ep in range(100):\n",
    "    #print(ep+1)\n",
    "    hist=model.fit(x=np.array(rdd_x.collect()), #<--- new data every time collect() is called\n",
    "                   y=np.array(rdd_y.collect()), \n",
    "                   batch_size=32, epochs=20,verbose=0)\n",
    "    lossv.extend(hist.history['loss'])\n",
    "plt.plot(lossv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d225d2e-b8d0-4ebd-a789-ade5b00fa7fa",
   "metadata": {},
   "source": [
    "# Check for convergence\n",
    "\n",
    "check if estimated parameters are close enough to ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14df287f-d8f9-452b-b498-820565544f3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Estimate: [3.] 3.9999743\n",
      "Ground Truth: [3] 4\n",
      "Delta: [0.] 2.574920654296875e-05\n"
     ]
    }
   ],
   "source": [
    "model_w8 = model.get_weights()\n",
    "est_m, est_c = model_w8[0][0],model_w8[1][0]\n",
    "print('Model Estimate:', est_m, est_c)\n",
    "print('Ground Truth:','['+str(ground_m)+']', ground_c)\n",
    "\n",
    "print('Delta:',ground_m-est_m, ground_c-est_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7f6303-7baf-4cdf-a07e-3cd4abfe6e40",
   "metadata": {},
   "source": [
    "# Using spark DataFrames in general"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492ab9b5-d191-44fc-a431-0ac3e4cbde30",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9851b493-015f-4f48-abe0-1f676a90a4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'> 1596\n",
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: double (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n",
      "+----------+-----------------+-----------------+-----------------+-----------------+----------+------------------+\n",
      "|      Date|             High|              Low|             Open|            Close|    Volume|         Adj Close|\n",
      "+----------+-----------------+-----------------+-----------------+-----------------+----------+------------------+\n",
      "|2010-08-02|9.378213882446289|9.272143363952637| 9.30142879486084|9.351785659790039|4.280556E8| 8.029596328735352|\n",
      "|2010-08-03|9.402142524719238|9.265000343322754|9.321785926818848|9.354642868041992|4.176536E8| 8.032052993774414|\n",
      "|2010-08-04|9.438570976257324|9.296786308288574|  9.3871431350708|9.392143249511719|4.203752E8| 8.064249038696289|\n",
      "|2010-08-05|9.399286270141602|9.305356979370117| 9.34749984741211|9.346428871154785|2.890972E8| 8.024996757507324|\n",
      "|2010-08-06|9.338929176330566|9.201070785522461|9.277856826782227|9.288928985595703|4.448976E8| 7.975627899169922|\n",
      "|2010-08-09|9.362500190734863|9.270357131958008|9.338570594787598|9.348214149475098| 3.03128E8| 8.026529312133789|\n",
      "|2010-08-10|9.301786422729492|9.198213577270508|9.280357360839844|9.264642715454102|  4.5192E8| 7.954774856567383|\n",
      "|2010-08-11|9.131786346435547|8.921786308288574|9.121429443359375|8.935357093811035|6.200544E8| 7.672046184539795|\n",
      "|2010-08-12|9.039285659790039|8.789999961853027|8.810357093811035|8.992500305175781|5.349204E8|7.7211103439331055|\n",
      "|2010-08-13| 8.99571418762207|8.896071434020996|8.987500190734863|8.896429061889648|3.548692E8| 7.638622283935547|\n",
      "|2010-08-16|8.928929328918457|8.807856559753418|8.842143058776855| 8.84428596496582|  3.1843E8|7.5938496589660645|\n",
      "|2010-08-17|9.093929290771484|8.899999618530273|8.931428909301758|8.998929023742676|4.226404E8| 7.726629257202148|\n",
      "|2010-08-18|9.095356941223145|8.984999656677246|9.012857437133789|9.038213729858398| 3.39696E8|7.7603607177734375|\n",
      "|2010-08-19|9.052857398986816|8.881428718566895|9.029999732971191|8.924285888671875| 4.26706E8| 7.662538051605225|\n",
      "|2010-08-20|9.068571090698242| 8.89285659790039| 8.90678596496582|8.915714263916016|  3.8423E8| 7.655179500579834|\n",
      "|2010-08-23|              9.0|8.758929252624512|8.992500305175781|8.778571128845215|4.140416E8|7.5374250411987305|\n",
      "|2010-08-24|8.678570747375488|8.523214340209961|8.666786193847656|8.568928718566895|6.025656E8| 7.357422828674316|\n",
      "|2010-08-25|8.713929176330566|8.471428871154785|8.501428604125977|8.674642562866211|5.968676E8| 7.448192596435547|\n",
      "|2010-08-26|8.776785850524902|8.581428527832031|8.766071319580078|8.581428527832031|4.665052E8| 7.368154048919678|\n",
      "|2010-08-27|8.664643287658691|8.412857055664062|8.633929252624512| 8.62928581237793|5.483912E8| 7.409246921539307|\n",
      "+----------+-----------------+-----------------+-----------------+-----------------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = spark.read.option('header',\n",
    "                            'true' #<---- the csv has a header\n",
    "                           ).csv(\"Apple_stock.csv\", \n",
    "                                inferSchema=True) # add infer schema to load in proper data type\n",
    "\n",
    "print(type(dataset), dataset.count())\n",
    "dataset.printSchema()\n",
    "dataset.show() # or use .describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbd93226-37c5-4dc1-ad0c-fa05624011be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      Date|    Volume|\n",
      "+----------+----------+\n",
      "|2010-08-02|4.280556E8|\n",
      "|2010-08-03|4.176536E8|\n",
      "|2010-08-04|4.203752E8|\n",
      "|2010-08-05|2.890972E8|\n",
      "|2010-08-06|4.448976E8|\n",
      "|2010-08-09| 3.03128E8|\n",
      "|2010-08-10|  4.5192E8|\n",
      "|2010-08-11|6.200544E8|\n",
      "|2010-08-12|5.349204E8|\n",
      "|2010-08-13|3.548692E8|\n",
      "|2010-08-16|  3.1843E8|\n",
      "|2010-08-17|4.226404E8|\n",
      "|2010-08-18| 3.39696E8|\n",
      "|2010-08-19| 4.26706E8|\n",
      "|2010-08-20|  3.8423E8|\n",
      "|2010-08-23|4.140416E8|\n",
      "|2010-08-24|6.025656E8|\n",
      "|2010-08-25|5.968676E8|\n",
      "|2010-08-26|4.665052E8|\n",
      "|2010-08-27|5.483912E8|\n",
      "+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.select(['Date','Volume']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2c9dc0-93bf-445e-b3e4-1448147be7b4",
   "metadata": {},
   "source": [
    "# Filter Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "884de2f8-22e4-4ef3-8ec3-36b21e511ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------------+\n",
      "|      Date|    Volume|         Adj Close|\n",
      "+----------+----------+------------------+\n",
      "|2010-08-02|4.280556E8| 8.029596328735352|\n",
      "|2010-08-03|4.176536E8| 8.032052993774414|\n",
      "|2010-08-04|4.203752E8| 8.064249038696289|\n",
      "|2010-08-06|4.448976E8| 7.975627899169922|\n",
      "|2010-08-10|  4.5192E8| 7.954774856567383|\n",
      "|2010-08-11|6.200544E8| 7.672046184539795|\n",
      "|2010-08-12|5.349204E8|7.7211103439331055|\n",
      "|2010-08-17|4.226404E8| 7.726629257202148|\n",
      "|2010-08-19| 4.26706E8| 7.662538051605225|\n",
      "|2010-08-23|4.140416E8|7.5374250411987305|\n",
      "|2010-08-24|6.025656E8| 7.357422828674316|\n",
      "|2010-08-25|5.968676E8| 7.448192596435547|\n",
      "|2010-08-26|4.665052E8| 7.368154048919678|\n",
      "|2010-08-27|5.483912E8| 7.409246921539307|\n",
      "|2010-08-31|4.207868E8|7.4546308517456055|\n",
      "|2010-09-01|6.970376E8|7.6763386726379395|\n",
      "|2010-09-02|4.154276E8| 7.732760429382324|\n",
      "|2010-09-03|5.207888E8| 7.935146808624268|\n",
      "|2010-09-08|5.265512E8| 8.062408447265625|\n",
      "|2010-09-09|4.385752E8| 8.067007064819336|\n",
      "+----------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.filter(\"Volume>=400000000\").select([\"Date\",\"Volume\",\"Adj Close\"]).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
