{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FstU8UmGtbIl",
    "outputId": "acfc37db-a567-4705-ae53-d5ae92a1ad77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Loading drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bzRIbfPCtegH",
    "outputId": "d79cb069-fd8c-465a-bbdf-eae5329d621a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w26bupBKtgl_",
    "outputId": "d354efad-535a-4cf3-eb06-1122143cd296"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive  sample_data\n",
      "/content/drive/My Drive/ML_Assignment\n",
      "/content/drive/My Drive/ML_Assignment\n"
     ]
    }
   ],
   "source": [
    "# Setting the working directory \n",
    "!ls\n",
    "%cd drive/My\\ Drive/ML_Assignment\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ky9_40xbthw9",
    "outputId": "d0e4916a-5dbe-4172-dbf1-4e1707633f7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-19 15:02:14--  https://www.dropbox.com/s/tp3l54tnatvbldf/bbc.csv?dl=0\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.80.18, 2620:100:601b:18::a27d:812\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.80.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/raw/tp3l54tnatvbldf/bbc.csv [following]\n",
      "--2021-11-19 15:02:15--  https://www.dropbox.com/s/raw/tp3l54tnatvbldf/bbc.csv\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc011dd9547997ca36b2ec7feb39.dl.dropboxusercontent.com/cd/0/inline/BaRlaSHaoWJ0BGN7_ectOQNhLLE2MnH_e-BZ-WrJYfSQxnkjKliyPMomNXpcyvm3qJWTZ7d0fM8CNAApBYVRehz5IIwXc72H6Y25CPlnJ7WQezMqzQvKQ7bse12cbG7Cv86JaWrU6OfPMPy_UGt1D9DZ/file# [following]\n",
      "--2021-11-19 15:02:16--  https://uc011dd9547997ca36b2ec7feb39.dl.dropboxusercontent.com/cd/0/inline/BaRlaSHaoWJ0BGN7_ectOQNhLLE2MnH_e-BZ-WrJYfSQxnkjKliyPMomNXpcyvm3qJWTZ7d0fM8CNAApBYVRehz5IIwXc72H6Y25CPlnJ7WQezMqzQvKQ7bse12cbG7Cv86JaWrU6OfPMPy_UGt1D9DZ/file\n",
      "Resolving uc011dd9547997ca36b2ec7feb39.dl.dropboxusercontent.com (uc011dd9547997ca36b2ec7feb39.dl.dropboxusercontent.com)... 162.125.64.15, 2620:100:601b:15::a27d:80f\n",
      "Connecting to uc011dd9547997ca36b2ec7feb39.dl.dropboxusercontent.com (uc011dd9547997ca36b2ec7feb39.dl.dropboxusercontent.com)|162.125.64.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4494967 (4.3M) [text/plain]\n",
      "Saving to: ‘bbc.csv’\n",
      "\n",
      "bbc.csv             100%[===================>]   4.29M  5.13MB/s    in 0.8s    \n",
      "\n",
      "2021-11-19 15:02:17 (5.13 MB/s) - ‘bbc.csv’ saved [4494967/4494967]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Downloading the data\n",
    "!wget https://www.dropbox.com/s/tp3l54tnatvbldf/bbc.csv?dl=0 -O 'bbc.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "kzAly4H0tjQ4"
   },
   "outputs": [],
   "source": [
    "# All general imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Embedding, Reshape, Conv2D, MaxPool2D, Concatenate, Flatten, Dropout, Dense, Bidirectional, GlobalAveragePooling1D, GRU, GlobalMaxPooling1D, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import LSTM, GRU, Conv1D, MaxPool1D, Activation, Add\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import SpatialDropout1D\n",
    "\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, Conv1D, Softmax\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import io, os, gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Szy8XB3XtkYp",
    "outputId": "1ed8d080-8a3b-4445-f1ac-70db8cb04970"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'Article', 'Class'], dtype='object')\n",
      "{'business': 0, 'entertainment': 1, 'politics': 2, 'sport': 3, 'tech': 4}\n",
      "   Unnamed: 0                                            Article     Class\n",
      "0           0  Ad sales boost Time Warner profit\\n\\nQuarterly...  business\n",
      "1           1  Dollar gains on Greenspan speech\\n\\nThe dollar...  business\n",
      "2           2  Yukos unit buyer faces loan claim\\n\\nThe owner...  business\n",
      "3           3  High fuel prices hit BA's profits\\n\\nBritish A...  business\n",
      "4           4  Pernod takeover talk lifts Domecq\\n\\nShares in...  business\n"
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "df = pd.read_csv(\"bbc.csv\")\n",
    "print(df.columns)\n",
    "le = LabelEncoder()\n",
    "input_labels = le.fit_transform(df['Class'])\n",
    "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(le_name_mapping)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5OOxTmB1tmEO"
   },
   "outputs": [],
   "source": [
    "# Pre-processing data\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "import re\n",
    "porter_stemmer = PorterStemmer()\n",
    "doc_list = df['Article'].tolist()\n",
    "processed_list = []\n",
    "for doc in doc_list:\n",
    "  doc_str = re.sub(r'[^\\w\\s]', '', remove_stopwords(doc))\n",
    "  doc_str_stem_lst = [porter_stemmer.stem(word) for word in doc_str.split(\" \")]\n",
    "  doc_str_stem = \" \".join(doc_str_stem_lst)\n",
    "  processed_list.append(doc_str_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "quVRfQaGtrzr"
   },
   "outputs": [],
   "source": [
    "# Defining the tokenizer\n",
    "def get_tokenizer(data):\n",
    "  print('Training tokenizer...')\n",
    "  tokenizer = Tokenizer()\n",
    "  print('Read {} Sentences'.format(len(data)))\n",
    "  tokenizer.fit_on_texts(data)\n",
    "  return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "s234QX1ltrQB"
   },
   "outputs": [],
   "source": [
    "def get_data(tokenizer, MAX_LENGTH, input_data, input_labels):\n",
    "  print('Loading data')\n",
    "  \n",
    "  assert len(input_data) == len(input_labels)\n",
    "  sequences = tokenizer.texts_to_sequences(input_data)\n",
    "  X = pad_sequences(sequences, maxlen=MAX_LENGTH)\n",
    "  Y_bcc = np.array(input_labels)\n",
    "\n",
    "  return X, Y_bcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QRJO_XhfuQ37",
    "outputId": "de6fa44f-60fb-48c2-9ebe-bc835c75c15e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer...\n",
      "Read 1912 Sentences\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(processed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eIqvkejludxY",
    "outputId": "4c237573-acb5-4d98-b6e7-d08df9492f6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 50\n",
    "# read ml data\n",
    "X, Y_bcc = get_data(tokenizer, MAX_LENGTH, processed_list, input_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wH8c93c-upWP",
    "outputId": "98e5ebaf-27c8-4e09-9077-e06c4de5e507"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Creating one-hot encodings\n",
    "y_bcc_labels = keras.utils.np_utils.to_categorical(Y_bcc)\n",
    "print(y_bcc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "XXq7MqB3u8Jh"
   },
   "outputs": [],
   "source": [
    "# Splitting data into train, val and test\n",
    "train1_X, test_X, train1_Y, test_Y = train_test_split(X, y_bcc_labels, test_size=0.2, random_state=43)\n",
    "train_X, val_X, train_Y, val_Y = train_test_split(train1_X, train1_Y, test_size=0.1, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "M5SYQf2DvffL"
   },
   "outputs": [],
   "source": [
    "# Setting hyper-parameters\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = MAX_LENGTH\n",
    "\n",
    "MAX_NUM_WORDS = len(tokenizer.word_index) + 1\n",
    "\n",
    "NUM_EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w8jvhwv2wkFV",
    "outputId": "a963fe26-7109-4ea4-8583-3b6c3c61e32f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Text FF NN\n",
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_14 (InputLayer)       [(None, 50)]              0         \n",
      "                                                                 \n",
      " embedding_13 (Embedding)    (None, 50, 100)           2253200   \n",
      "                                                                 \n",
      " flatten_13 (Flatten)        (None, 5000)              0         \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 2048)              10242048  \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,120,789\n",
      "Trainable params: 15,120,789\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Defining the model\n",
    "print('Getting Text FF NN')\n",
    "input_layer = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
    "    dtype='int32')\n",
    "embedding_layer = Embedding(MAX_NUM_WORDS, NUM_EMBEDDING_DIM, trainable=True)\n",
    "embedded_input = embedding_layer(input_layer)\n",
    "flatten_input = Flatten()(embedded_input)\n",
    "dense_layer_1 = Dense(2048, activation='relu')\n",
    "drop_1 = Dropout(0.1)\n",
    "dense_layer_2 = Dense(1024, activation='relu')\n",
    "drop_2 = Dropout(0.1)\n",
    "dense_layer_3 = Dense(512, activation='relu')\n",
    "drop_3 = Dropout(0.1)\n",
    "dense_layer_4 = Dense(5, activation='softmax')\n",
    "output_layer = dense_layer_4(drop_3(dense_layer_3(drop_2(dense_layer_2(drop_1(dense_layer_1(flatten_input)))))))\n",
    "model = Model(\n",
    "    inputs=input_layer, \n",
    "    outputs=output_layer)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "S515ZVvlx4cv"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "lr = 1e-3\n",
    "opt = Adam(learning_rate=lr, decay=lr/50)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wygsBcAf6JtF",
    "outputId": "840675f8-3103-4de1-8d98-f4fa67df29eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/3 [==============================] - 1s 146ms/step - loss: 1.5757 - accuracy: 0.2594 - val_loss: 1.5599 - val_accuracy: 0.2810\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 1.4078 - accuracy: 0.3510 - val_loss: 1.4653 - val_accuracy: 0.4052\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.8927 - accuracy: 0.8779 - val_loss: 1.2417 - val_accuracy: 0.5294\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.2866 - accuracy: 0.9077 - val_loss: 1.0361 - val_accuracy: 0.6013\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.0993 - accuracy: 0.9978 - val_loss: 1.0769 - val_accuracy: 0.6471\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 100\n",
    "stop = [EarlyStopping(monitor='val_loss', patience=1)]\n",
    "history = model.fit(x=train_X,\n",
    "                    y=train_Y,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=(\n",
    "                      val_X, \n",
    "                      val_Y\n",
    "                    ),\n",
    "                    shuffle=True,\n",
    "                    callbacks=stop,\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "6BCB-W80_Nur"
   },
   "outputs": [],
   "source": [
    "# Getting predictions\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "predictions = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T5Vcpj2p_ZNP",
    "outputId": "13a7eb08-c8b3-4fd6-a537-e6e89cf47f95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBC News Accuracy is\n",
      "57.180156657963444\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.55      0.80      0.65        79\n",
      "entertainment       0.59      0.68      0.64        69\n",
      "     politics       0.75      0.56      0.64        93\n",
      "        sport       0.28      0.38      0.32        47\n",
      "         tech       0.71      0.41      0.52        95\n",
      "\n",
      "     accuracy                           0.57       383\n",
      "    macro avg       0.58      0.57      0.55       383\n",
      " weighted avg       0.61      0.57      0.57       383\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = [idx for idx in np.argmax(predictions, axis=1)]\n",
    "y_true = [idx for idx in np.argmax(test_Y, axis=1)]\n",
    "print('BBC News Accuracy is')\n",
    "print(metrics.accuracy_score(y_true, y_pred)*100)\n",
    "print(classification_report(y_true, y_pred, target_names = list(le_name_mapping.keys())))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "BBC_News.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
