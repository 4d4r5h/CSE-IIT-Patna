{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Imports\r\n",
    "import re\r\n",
    "import math\r\n",
    "from nltk.stem import WordNetLemmatizer # pip install nltk\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from collections import Counter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Read and process data\r\n",
    "def process_data(text):\r\n",
    "    no_punct_string = re.sub(r'[^\\w\\s]', '', text)\r\n",
    "    return no_punct_string.lower()\r\n",
    "    \r\n",
    "with open('SMSSpamCollection', 'r') as infine:\r\n",
    "    lines = infine.readlines()\r\n",
    "    lines = [line.rstrip() for line in lines]\r\n",
    "    labels = []\r\n",
    "    data = []\r\n",
    "    for line in lines:\r\n",
    "        line_contents = line.split('\\t')\r\n",
    "        labels.append(line_contents[0])\r\n",
    "        data.append(process_data(line_contents[1]))\r\n",
    "print('Unique labels are', set(labels))\r\n",
    "print('Len of data is', len(data))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unique labels are {'ham', 'spam'}\n",
      "Len of data is 5574\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Class for Multinomial Naive Bayes \r\n",
    "class MultiNomialNB():\r\n",
    "    def __init__(self, spam_data, ham_data):\r\n",
    "        self.cnt_spam, self.cnt_ham = self.build_counts(spam_data, ham_data)\r\n",
    "        self.spam_words_count = self.get_count(self.cnt_spam)  # Total number of features in the spam class\r\n",
    "        self.ham_words_count = self.get_count(self.cnt_ham) # Total number of features in the ham class\r\n",
    "        self.vocab_count = len(self.cnt_spam) + len(self.cnt_ham) # Total number of words accross all documents\r\n",
    "        self.spam_count = len(spam_data) # Total number of spam documents\r\n",
    "        self.ham_count = len(ham_data)\r\n",
    "        self.doc_count = self.spam_count + self.ham_count # Total number of all features\r\n",
    "\r\n",
    "    def get_count(self, count_dict):\r\n",
    "        \"\"\"\r\n",
    "        Gives the total number of words for a given class\r\n",
    "        \"\"\"\r\n",
    "        tot_count = 0\r\n",
    "        for key, value in count_dict.items():\r\n",
    "            tot_count += value\r\n",
    "        return tot_count\r\n",
    "\r\n",
    "    def build_counts(self, spam_data, ham_data):\r\n",
    "        \"\"\"\r\n",
    "        Builds the feature dictionary for every class\r\n",
    "        \"\"\"\r\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\r\n",
    "        stops = set(stopwords.words('english'))\r\n",
    "        cnt_spam = Counter()\r\n",
    "        cnt_ham = Counter()\r\n",
    "        for spam_texts in spam_data:\r\n",
    "            spam_words = [wordnet_lemmatizer.lemmatize(word) for word in spam_texts.split(' ') if word not in stops and word is not '']\r\n",
    "            for word in spam_words:\r\n",
    "                cnt_spam[word] += 1\r\n",
    "        for ham_texts in ham_data:\r\n",
    "            ham_words = [wordnet_lemmatizer.lemmatize(word) for word in ham_texts.split(' ') if word not in stops and word is not '']\r\n",
    "            for word in ham_words:\r\n",
    "                cnt_ham[word] += 1\r\n",
    "        return cnt_spam, cnt_ham\r\n",
    "    def train(self):\r\n",
    "        \"\"\"\r\n",
    "        Creates the feature vector for each class consisting of word -> smoothed proability of occurence\r\n",
    "        Also, computes the apriori probabilities for each class\r\n",
    "        \"\"\"\r\n",
    "        self.features = {}\r\n",
    "        self.features['spam_features'] = {}\r\n",
    "        self.features['ham_features'] = {}\r\n",
    "\r\n",
    "        # Setting the a priori class probablities\r\n",
    "        self.priorLogSpam = math.log(self.spam_count/self.doc_count)\r\n",
    "        self.priorLogHam = math.log(self.ham_count/self.doc_count)\r\n",
    "\r\n",
    "        # Probablity of each feature in each class\r\n",
    "        for word, count in self.cnt_spam.items():\r\n",
    "            self.features['spam_features'][word] = math.log((count+1)/(self.spam_words_count+self.vocab_count))\r\n",
    "        for word, count in self.cnt_ham.items():\r\n",
    "            self.features['ham_features'][word] = math.log((count+1)/(self.ham_words_count+self.vocab_count))\r\n",
    "\r\n",
    "    def test(self, document):\r\n",
    "        \"\"\"\r\n",
    "        Takes a document and predicts whether spam or ham\r\n",
    "        \"\"\"\r\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\r\n",
    "        stops = set(stopwords.words('english'))\r\n",
    "        document = [wordnet_lemmatizer.lemmatize(x) for x in document.split(\" \") if x not in stops and x is not '']\r\n",
    "        spam_val = self.priorLogSpam\r\n",
    "        ham_val = self.priorLogHam\r\n",
    "\r\n",
    "        # Initializing the smooth probabilites\r\n",
    "        smooth_spam = math.log(1/(self.spam_words_count+self.doc_count))\r\n",
    "        smooth_ham = math.log(1/(self.ham_words_count+self.doc_count))\r\n",
    "\r\n",
    "        # Updating the scores for each class\r\n",
    "        # Spam Class\r\n",
    "        for word in document:\r\n",
    "            if word in self.features['spam_features']:\r\n",
    "                spam_val += self.features['spam_features'][word]\r\n",
    "            elif word in self.features['ham_features']:\r\n",
    "                spam_val += smooth_spam\r\n",
    "        # Ham Class\r\n",
    "        for word in document:\r\n",
    "            if word in self.features['ham_features']:\r\n",
    "                ham_val += self.features['ham_features'][word]\r\n",
    "            elif word in self.features['spam_features']:\r\n",
    "                ham_val += smooth_ham\r\n",
    "        if spam_val >= ham_val:\r\n",
    "            return ('spam', spam_val)\r\n",
    "        else:\r\n",
    "            return ('ham', ham_val)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#TODO: Implement k-fold cross-validation code\r\n",
    "kfold = 5\r\n",
    "fold_size = len(data)//kfold\r\n",
    "fold_indices = []\r\n",
    "for fold in range(kfold):\r\n",
    "    fold_indices.append(fold*fold_size)\r\n",
    "fold_indices.append(len(data))\r\n",
    "print('K-Fold indices', fold_indices)\r\n",
    "\r\n",
    "def get_kfold_data(fold):\r\n",
    "    test_range = fold_indices[fold], fold_indices[fold+1]\r\n",
    "    test_data, test_labels = data[test_range[0]:test_range[1]], labels[test_range[0]:test_range[1]]\r\n",
    "    train_data, train_labels = [], []\r\n",
    "    for i in range(kfold):\r\n",
    "        if fold == i:\r\n",
    "            continue\r\n",
    "        else:\r\n",
    "            train_range = fold_indices[i], fold_indices[i+1]\r\n",
    "            train_data.extend(data[train_range[0]:train_range[1]])\r\n",
    "            train_labels.extend(labels[train_range[0]:train_range[1]])\r\n",
    "    assert len(train_data) + len(test_data) == len(data)\r\n",
    "    return (train_data, train_labels), (test_data, test_labels)\r\n",
    "\r\n",
    "def get_train_ham_spam(train_data, train_labels):\r\n",
    "    # Splitting data into spam and ham\r\n",
    "    train_spam_data = []\r\n",
    "    train_ham_data = []\r\n",
    "    for i, text in enumerate(train_data):\r\n",
    "        if train_labels[i] == 'spam':\r\n",
    "            train_spam_data.append(text)\r\n",
    "        elif train_labels[i] == 'ham':\r\n",
    "            train_ham_data.append(text)\r\n",
    "        else:\r\n",
    "            print('Labels not extracted properly')\r\n",
    "    assert len(train_spam_data) + len(train_ham_data) == len(train_data)\r\n",
    "    return train_spam_data, train_ham_data"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "K-Fold indices [0, 1114, 2228, 3342, 4456, 5574]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Auxilliary Functions\r\n",
    "# Getting predictions on the test set\r\n",
    "def get_predictions(test_data, multinb_classifer):\r\n",
    "    pred_labels = []\r\n",
    "    pred_scores = []\r\n",
    "    for test_doc in test_data:\r\n",
    "        pred_label, pred_score = multinb_classifer.test(test_doc)\r\n",
    "        pred_labels.append(pred_label)\r\n",
    "        pred_scores.append(pred_score)\r\n",
    "    return pred_labels, pred_scores\r\n",
    "\r\n",
    "def get_accuracy(pred_labels, test_labels):\r\n",
    "    # Calculating accuracy\r\n",
    "    correct = 0\r\n",
    "    wrong_indices = []\r\n",
    "    for i, pred_label in enumerate(pred_labels):\r\n",
    "        if pred_label == test_labels[i]:\r\n",
    "            correct += 1\r\n",
    "        else:\r\n",
    "            wrong_indices.append(i)\r\n",
    "    accuracy = (correct/len(pred_labels))*100\r\n",
    "    return accuracy\r\n",
    "\r\n",
    "def print_classification_matrix(pred_labels, test_labels):\r\n",
    "    orig_spam = 0\r\n",
    "    orig_ham = 0\r\n",
    "    for label in test_labels:\r\n",
    "        if label == 'spam':\r\n",
    "            orig_spam += 1\r\n",
    "        elif label == 'ham':\r\n",
    "            orig_ham += 1\r\n",
    "    spam_as_ham = 0\r\n",
    "    ham_as_spam = 0\r\n",
    "    for i, pred_label in enumerate(pred_labels):\r\n",
    "        if pred_label != test_labels[i]:\r\n",
    "            if test_labels[i] == 'spam':\r\n",
    "                spam_as_ham += 1\r\n",
    "            elif test_labels[i] == 'ham':\r\n",
    "                ham_as_spam += 1\r\n",
    "    spam_as_spam = orig_spam - spam_as_ham\r\n",
    "    ham_as_ham = orig_ham - ham_as_spam\r\n",
    "    print(\"Classification Matrix\\n\")\r\n",
    "    print(\"\\tSpam\\tHam\")\r\n",
    "    print(\"Spam\\t\", spam_as_spam, \"\\t\", spam_as_ham)\r\n",
    "    print(\"Ham\\t\", ham_as_spam, \"\\t\", ham_as_ham)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Implementing k-fold\r\n",
    "total_pred_labels = []\r\n",
    "total_test_labels = []\r\n",
    "for fold in range(kfold):\r\n",
    "    (train_data, train_labels), (test_data, test_labels) = get_kfold_data(fold)\r\n",
    "    train_spam_data, train_ham_data = get_train_ham_spam(train_data, train_labels)\r\n",
    "    multinb_classifer = MultiNomialNB(train_spam_data, train_ham_data)\r\n",
    "    multinb_classifer.train() # Training  \r\n",
    "    pred_labels, pred_scores = get_predictions(test_data, multinb_classifer) # Predicting on the test set\r\n",
    "    total_pred_labels.extend(pred_labels)\r\n",
    "    total_test_labels.extend(test_labels)\r\n",
    "# Calcuating the accuracy \r\n",
    "accuracy = get_accuracy(total_pred_labels, total_test_labels)\r\n",
    "print_classification_matrix(total_pred_labels, total_test_labels)\r\n",
    "print('Accuracy of k-Fold cross-validated Multinomial Naive Bayes is', accuracy)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classification Matrix\n",
      "\n",
      "\tSpam\tHam\n",
      "Spam\t 673 \t 74\n",
      "Ham\t 30 \t 4797\n",
      "Accuracy of k-Fold cross-validated Multinomial Naive Bayes is 98.13419447434516\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "4905652b14e4b7eb92899b78ac499a22c488804455b27940a322fd82aaf71031"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}