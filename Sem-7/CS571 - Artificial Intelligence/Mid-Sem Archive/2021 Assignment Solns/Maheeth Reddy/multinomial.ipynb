{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CS561/571 Artificial Intelligence\n",
    "## Mid Semester Assignment\n",
    "\n",
    "### Multinomial Model of Naive Bayes Classifier\n",
    "\n",
    "<table style=\"font-size:15px\">\n",
    "    <thead>\n",
    "        <td><b>Name of Student</b></td>\n",
    "        <td><b>Roll No.</b></td>\n",
    "        <td><b>Date</b></td>\n",
    "    </thead>\n",
    "    <tr>\n",
    "        <td>M. Maheeth Reddy</td>\n",
    "        <td>1801CS31</td>\n",
    "        <td>22-Sep-2021</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "**NOTE**: Before running this notebook, Please execute the following commands in Python shell if you have not performed them before\n",
    "```python3\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Importing Libraries\n",
    "import math\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "# Please perform above mentioned commands before running this notebook\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Function to pre-process the dataset\n",
    "# basically removing punctuation marks \n",
    "# and converting text to lower case\n",
    "def preprocess_data(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "    \n",
    "# Function to read, pre-process and \n",
    "# identify unique labels in the data\n",
    "def read_dataset(dataset_name):\n",
    "    with open(dataset_name, 'r') as dataset:\n",
    "        lines = dataset.readlines()\n",
    "        lines = [line.rstrip() for line in lines]\n",
    "\n",
    "        data, labels = [], []\n",
    "        for line in lines:\n",
    "            label_data = line.split('->')\n",
    "            labels.append(label_data[0])\n",
    "            data.append(preprocess_data(label_data[1]))\n",
    "        \n",
    "    return data, labels\n",
    "\n",
    "data, labels = read_dataset('dataset_file')\n",
    "# Print the labels in the dataset \n",
    "# and number of items in the dataset\n",
    "print('We have these labels in the dataset: ', ', '.join(set(labels)))\n",
    "print(f'There are {len(data)} items in the dataset')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "We have these labels in the dataset:  DL, CV\n",
      "There are 3 items in the dataset\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Python Class to implement Multinomial Naive Bayes Classifier\n",
    "class Classifier():\n",
    "    # Constructor function\n",
    "    def __init__(self, dl_data, cv_data):\n",
    "        # count of words in DL class\n",
    "        self.dl_word_count = self.get_counts(dl_data)\n",
    "\n",
    "        # count of words in CV class\n",
    "        self.cv_word_count = self.get_counts(cv_data)\n",
    "        \n",
    "        # Total number of features in the DL class\n",
    "        self.dl_total_words = sum([value for _,value in self.dl_word_count.items()])\n",
    "        \n",
    "        # Total number of features in the CV class\n",
    "        self.cv_total_words = sum([value for _,value in self.cv_word_count.items()])\n",
    "        \n",
    "        # Total number of words accross all documents\n",
    "        self.words_count = len(self.dl_word_count) + len(self.cv_word_count)\n",
    "        \n",
    "        # Total number of DL documents\n",
    "        self.dl_doc_count = len(dl_data)\n",
    "        \n",
    "        # Total number of CV documents\n",
    "        self.cv_doc_count = len(cv_data)\n",
    "        \n",
    "        # Total number of all features\n",
    "        self.doc_count = self.dl_doc_count + self.cv_doc_count\n",
    "\n",
    "        # Setting the a priori class probablities\n",
    "        self.prior_prob_dl = math.log(self.dl_doc_count/self.doc_count)\n",
    "        self.prior_prob_cv = math.log(self.cv_doc_count/self.doc_count)\n",
    "\n",
    "    # get counts of each word from data of a class\n",
    "    def get_counts(self, class_data):\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        stopwordset = set(stopwords.words('english'))\n",
    "        \n",
    "        class_word_count = Counter()\n",
    "        for class_texts in class_data:\n",
    "            class_words = []\n",
    "            for word in class_texts.split(' '):\n",
    "                if word not in stopwordset and word != '':\n",
    "                    lemmatized = wordnet_lemmatizer.lemmatize(word)\n",
    "                    class_words.append(lemmatized)\n",
    "            \n",
    "            for word in class_words:\n",
    "                class_word_count[word] += 1\n",
    "        \n",
    "        return class_word_count\n",
    "        \n",
    "    # Creates the feature vector for DL and CV class\n",
    "    def train(self):\n",
    "        self.features = {}\n",
    "        self.features['dl_features'] = {}\n",
    "        self.features['cv_features'] = {}\n",
    "\n",
    "        # Probablity of each word in DL class\n",
    "        for word, count in self.dl_word_count.items():\n",
    "            prob = (count + 1)/(self.dl_total_words + self.words_count)\n",
    "            feat_val = math.log(prob)\n",
    "            self.features['dl_features'][word] = feat_val\n",
    "        \n",
    "        # Probablity of each word in CV class\n",
    "        for word, count in self.cv_word_count.items():\n",
    "            prob = (count + 1)/(self.cv_total_words + self.words_count)\n",
    "            feat_val = math.log(prob)\n",
    "            self.features['cv_features'][word] = feat_val\n",
    "\n",
    "    # predicts class for a test document\n",
    "    def test(self, document):\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        stops = set(stopwords.words('english'))\n",
    "        \n",
    "        document_ = []\n",
    "        for x in document.split(\" \"):\n",
    "            if x not in stops and x != '':\n",
    "                lemmatize = wordnet_lemmatizer.lemmatize(x)\n",
    "                document_.append(lemmatize)\n",
    "        document = document_\n",
    "        \n",
    "        dl_val = self.prior_prob_dl\n",
    "        cv_val = self.prior_prob_cv\n",
    "\n",
    "        # Initializing the smoothing probabilites\n",
    "        smooth_dl = math.log(1/(self.dl_total_words + self.doc_count))\n",
    "        smooth_cv = math.log(1/(self.cv_total_words + self.doc_count))\n",
    "\n",
    "        # Updating the scores for DL class\n",
    "        for word in document:\n",
    "            if word in self.features['dl_features']:\n",
    "                dl_val += self.features['dl_features'][word]\n",
    "            elif word in self.features['cv_features']:\n",
    "                dl_val += smooth_dl\n",
    "        \n",
    "        # Updating the scores for CV class\n",
    "        for word in document:\n",
    "            if word in self.features['cv_features']:\n",
    "                cv_val += self.features['cv_features'][word]\n",
    "            elif word in self.features['dl_features']:\n",
    "                cv_val += smooth_cv\n",
    "        \n",
    "        return (\"DL\", dl_val) if dl_val >= cv_val else (\"CV\", cv_val)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Splitting data into dl and cv\n",
    "def split_dataset(data, labels):\n",
    "    dl_data = []\n",
    "    cv_data = []\n",
    "    for i in range(len(data)):\n",
    "        if labels[i] == \"DL\":\n",
    "            dl_data.append(data[i])\n",
    "        elif labels[i] == \"CV\":\n",
    "            cv_data.append(data[i])\n",
    "        else:\n",
    "            print(\"Labels have not been extracted properly\")\n",
    "    \n",
    "    assert len(dl_data) + len(cv_data) == len(data)\n",
    "    return dl_data, cv_data\n",
    "\n",
    "# split the dataset into DL class and CV class\n",
    "dl_data, cv_data = split_dataset(data, labels)\n",
    "\n",
    "# create the classifier and train\n",
    "classifier = Classifier(dl_data, cv_data)\n",
    "classifier.train()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# predict the class for test document\n",
    "D4 = \"Deep learning based computer vision methods have been used for facial recognition.\"\n",
    "pred_label, _ = classifier.test(D4)\n",
    "print(f\"Document D4 has been classified into Class {pred_label}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Document D4 has been classified into Class CV\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# print the feature values for DL Class\n",
    "print(\"Feature Values for DL Class\\n\")\n",
    "for word,value in classifier.features['dl_features'].items():\n",
    "    print(f\"{word}: {value}\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Feature Values for DL Class\n",
      "\n",
      "recent: -3.5553480614894135\n",
      "year: -3.5553480614894135\n",
      "researcher: -3.5553480614894135\n",
      "computer: -3.5553480614894135\n",
      "vision: -3.5553480614894135\n",
      "proposed: -3.5553480614894135\n",
      "many: -3.5553480614894135\n",
      "deep: -2.8622008809294686\n",
      "learning: -3.1498829533812494\n",
      "dl: -3.5553480614894135\n",
      "method: -3.1498829533812494\n",
      "various: -3.5553480614894135\n",
      "task: -3.5553480614894135\n",
      "facial: -3.5553480614894135\n",
      "recognition: -3.5553480614894135\n",
      "fr: -3.1498829533812494\n",
      "made: -3.5553480614894135\n",
      "enormous: -3.5553480614894135\n",
      "leap: -3.5553480614894135\n",
      "using: -3.5553480614894135\n",
      "technique: -3.5553480614894135\n",
      "system: -3.5553480614894135\n",
      "benefit: -3.5553480614894135\n",
      "hierarchical: -3.5553480614894135\n",
      "architecture: -3.5553480614894135\n",
      "learn: -3.5553480614894135\n",
      "discriminative: -3.5553480614894135\n",
      "face: -3.5553480614894135\n",
      "representation: -3.5553480614894135\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# print the feature values for CV Class\n",
    "print(\"Feature Values for CV Class\\n\")\n",
    "for word,value in classifier.features['cv_features'].items():\n",
    "    print(f\"{word}: {value}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Feature Values for CV Class\n",
      "\n",
      "computer: -3.068052935133617\n",
      "vision: -3.068052935133617\n",
      "method: -3.068052935133617\n",
      "widely: -3.068052935133617\n",
      "used: -3.068052935133617\n",
      "facial: -3.068052935133617\n",
      "recognition: -3.068052935133617\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "3cb8c012cfbb4bd134dbec866004973183fdd2df9a4855f830bc1223b354bc0c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}