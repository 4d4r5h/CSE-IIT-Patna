{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import math\n",
    "from nltk.stem import WordNetLemmatizer # pip install nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a273b3bd145c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mline_contents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_contents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_contents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unique labels are'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Len of data is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Read and process data\n",
    "def process_data(text):\n",
    "    no_punct_string = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return no_punct_string.lower()\n",
    "    \n",
    "with open('dataset', 'r') as infine:\n",
    "    lines = infine.readlines()\n",
    "    lines = [line.rstrip() for line in lines]\n",
    "    labels = []\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        line_contents = line.split('\\t')\n",
    "        labels.append(line_contents[0])\n",
    "        data.append(process_data(line_contents[1]))\n",
    "print('Unique labels are', set(labels))\n",
    "print('Len of data is', len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for Multinomial Naive Bayes \n",
    "class MultiNomialNB():\n",
    "    def __init__(self, spam_data, ham_data):\n",
    "        self.cnt_spam, self.cnt_ham = self.build_counts(spam_data, ham_data)\n",
    "        self.spam_words_count = self.get_count(self.cnt_spam)  # Total number of features in the spam class\n",
    "        self.ham_words_count = self.get_count(self.cnt_ham) # Total number of features in the ham class\n",
    "        self.vocab_count = len(self.cnt_spam) + len(self.cnt_ham) # Total number of words accross all documents\n",
    "        self.spam_count = len(spam_data) # Total number of spam documents\n",
    "        self.ham_count = len(ham_data)\n",
    "        self.doc_count = self.spam_count + self.ham_count # Total number of all features\n",
    "\n",
    "    def get_count(self, count_dict):\n",
    "        \"\"\"\n",
    "        Gives the total number of words for a given class\n",
    "        \"\"\"\n",
    "        tot_count = 0\n",
    "        for key, value in count_dict.items():\n",
    "            tot_count += value\n",
    "        return tot_count\n",
    "\n",
    "    def build_counts(self, spam_data, ham_data):\n",
    "        \"\"\"\n",
    "        Builds the feature dictionary for every class\n",
    "        \"\"\"\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        stops = set(stopwords.words('english'))\n",
    "        cnt_spam = Counter()\n",
    "        cnt_ham = Counter()\n",
    "        for spam_texts in spam_data:\n",
    "            spam_words = [wordnet_lemmatizer.lemmatize(word) for word in spam_texts.split(' ') if word not in stops and word is not '']\n",
    "            for word in spam_words:\n",
    "                cnt_spam[word] += 1\n",
    "        for ham_texts in ham_data:\n",
    "            ham_words = [wordnet_lemmatizer.lemmatize(word) for word in ham_texts.split(' ') if word not in stops and word is not '']\n",
    "            for word in ham_words:\n",
    "                cnt_ham[word] += 1\n",
    "        return cnt_spam, cnt_ham\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Creates the feature vector for each class consisting of word -> smoothed proability of occurence\n",
    "        Also, computes the apriori probabilities for each class\n",
    "        \"\"\"\n",
    "        self.features = {}\n",
    "        self.features['spam_features'] = {}\n",
    "        self.features['ham_features'] = {}\n",
    "\n",
    "        # Setting the a priori class probablities\n",
    "        self.priorLogSpam = math.log(self.spam_count/self.doc_count)\n",
    "        self.priorLogHam = math.log(self.ham_count/self.doc_count)\n",
    "\n",
    "        # Probablity of each feature in each class\n",
    "        for word, count in self.cnt_spam.items():\n",
    "            self.features['spam_features'][word] = math.log((count+1)/(self.spam_words_count+self.vocab_count))\n",
    "        for word, count in self.cnt_ham.items():\n",
    "            self.features['ham_features'][word] = math.log((count+1)/(self.ham_words_count+self.vocab_count))\n",
    "\n",
    "    def test(self, document):\n",
    "        \"\"\"\n",
    "        Takes a document and predicts whether spam or ham\n",
    "        \"\"\"\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        stops = set(stopwords.words('english'))\n",
    "        document = [wordnet_lemmatizer.lemmatize(x) for x in document.split(\" \") if x not in stops and x is not '']\n",
    "        spam_val = self.priorLogSpam\n",
    "        ham_val = self.priorLogHam\n",
    "\n",
    "        # Initializing the smooth probabilites\n",
    "        smooth_spam = math.log(1/(self.spam_words_count+self.doc_count))\n",
    "        smooth_ham = math.log(1/(self.ham_words_count+self.doc_count))\n",
    "\n",
    "        # Updating the scores for each class\n",
    "        # Spam Class\n",
    "        for word in document:\n",
    "            if word in self.features['spam_features']:\n",
    "                spam_val += self.features['spam_features'][word]\n",
    "            elif word in self.features['ham_features']:\n",
    "                spam_val += smooth_spam\n",
    "        # Ham Class\n",
    "        for word in document:\n",
    "            if word in self.features['ham_features']:\n",
    "                ham_val += self.features['ham_features'][word]\n",
    "            elif word in self.features['spam_features']:\n",
    "                ham_val += smooth_ham\n",
    "        if spam_val >= ham_val:\n",
    "            return ('spam', spam_val)\n",
    "        else:\n",
    "            return ('ham', ham_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Implement k-fold cross-validation code\n",
    "kfold = 5\n",
    "fold_size = len(data)//kfold\n",
    "fold_indices = []\n",
    "for fold in range(kfold):\n",
    "    fold_indices.append(fold*fold_size)\n",
    "fold_indices.append(len(data))\n",
    "print('K-Fold indices', fold_indices)\n",
    "\n",
    "def get_kfold_data(fold):\n",
    "    test_range = fold_indices[fold], fold_indices[fold+1]\n",
    "    test_data, test_labels = data[test_range[0]:test_range[1]], labels[test_range[0]:test_range[1]]\n",
    "    train_data, train_labels = [], []\n",
    "    for i in range(kfold):\n",
    "        if fold == i:\n",
    "            continue\n",
    "        else:\n",
    "            train_range = fold_indices[i], fold_indices[i+1]\n",
    "            train_data.extend(data[train_range[0]:train_range[1]])\n",
    "            train_labels.extend(labels[train_range[0]:train_range[1]])\n",
    "    assert len(train_data) + len(test_data) == len(data)\n",
    "    return (train_data, train_labels), (test_data, test_labels)\n",
    "\n",
    "def get_train_ham_spam(train_data, train_labels):\n",
    "    # Splitting data into spam and ham\n",
    "    train_spam_data = []\n",
    "    train_ham_data = []\n",
    "    for i, text in enumerate(train_data):\n",
    "        if train_labels[i] == 'spam':\n",
    "            train_spam_data.append(text)\n",
    "        elif train_labels[i] == 'ham':\n",
    "            train_ham_data.append(text)\n",
    "        else:\n",
    "            print('Labels not extracted properly')\n",
    "    assert len(train_spam_data) + len(train_ham_data) == len(train_data)\n",
    "    return train_spam_data, train_ham_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxilliary Functions\n",
    "# Getting predictions on the test set\n",
    "def get_predictions(test_data, multinb_classifer):\n",
    "    pred_labels = []\n",
    "    pred_scores = []\n",
    "    for test_doc in test_data:\n",
    "        pred_label, pred_score = multinb_classifer.test(test_doc)\n",
    "        pred_labels.append(pred_label)\n",
    "        pred_scores.append(pred_score)\n",
    "    return pred_labels, pred_scores\n",
    "\n",
    "def get_accuracy(pred_labels, test_labels):\n",
    "    # Calculating accuracy\n",
    "    correct = 0\n",
    "    wrong_indices = []\n",
    "    for i, pred_label in enumerate(pred_labels):\n",
    "        if pred_label == test_labels[i]:\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong_indices.append(i)\n",
    "    accuracy = (correct/len(pred_labels))*100\n",
    "    return accuracy\n",
    "\n",
    "def print_classification_matrix(pred_labels, test_labels):\n",
    "    orig_spam = 0\n",
    "    orig_ham = 0\n",
    "    for label in test_labels:\n",
    "        if label == 'spam':\n",
    "            orig_spam += 1\n",
    "        elif label == 'ham':\n",
    "            orig_ham += 1\n",
    "    spam_as_ham = 0\n",
    "    ham_as_spam = 0\n",
    "    for i, pred_label in enumerate(pred_labels):\n",
    "        if pred_label != test_labels[i]:\n",
    "            if test_labels[i] == 'spam':\n",
    "                spam_as_ham += 1\n",
    "            elif test_labels[i] == 'ham':\n",
    "                ham_as_spam += 1\n",
    "    spam_as_spam = orig_spam - spam_as_ham\n",
    "    ham_as_ham = orig_ham - ham_as_spam\n",
    "    print(\"Classification Matrix\\n\")\n",
    "    print(\"\\tSpam\\tHam\")\n",
    "    print(\"Spam\\t\", spam_as_spam, \"\\t\", spam_as_ham)\n",
    "    print(\"Ham\\t\", ham_as_spam, \"\\t\", ham_as_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kfold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-98c699fe7a41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtotal_pred_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtotal_test_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_kfold_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_spam_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ham_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_ham_spam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kfold' is not defined"
     ]
    }
   ],
   "source": [
    "# Implementing k-fold\n",
    "total_pred_labels = []\n",
    "total_test_labels = []\n",
    "for fold in range(kfold):\n",
    "    (train_data, train_labels), (test_data, test_labels) = get_kfold_data(fold)\n",
    "    train_spam_data, train_ham_data = get_train_ham_spam(train_data, train_labels)\n",
    "    multinb_classifer = MultiNomialNB(train_spam_data, train_ham_data)\n",
    "    multinb_classifer.train() # Training  \n",
    "    pred_labels, pred_scores = get_predictions(test_data, multinb_classifer) # Predicting on the test set\n",
    "    total_pred_labels.extend(pred_labels)\n",
    "    total_test_labels.extend(test_labels)\n",
    "# Calcuating the accuracy \n",
    "accuracy = get_accuracy(total_pred_labels, total_test_labels)\n",
    "print_classification_matrix(total_pred_labels, total_test_labels)\n",
    "print('Accuracy of k-Fold cross-validated Multinomial Naive Bayes is', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4905652b14e4b7eb92899b78ac499a22c488804455b27940a322fd82aaf71031"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
